{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Tabular SSL Documentation","text":"<p>Welcome to the documentation for Tabular SSL, a modular library for self-supervised learning on tabular data with state-of-the-art corruption strategies. This documentation follows the Di\u00e1taxis framework to provide you with the most effective learning and reference experience.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>New to Tabular SSL? Try our interactive demos to see the library in action:</p> <pre><code># Demo corruption strategies (VIME, SCARF, ReConTab)\npython demo_corruption_strategies.py\n\n# Demo with real credit card transaction data\npython demo_credit_card_data.py\n\n# Train with state-of-the-art SSL methods\npython train.py +experiment=vime_ssl     # VIME approach\npython train.py +experiment=scarf_ssl    # SCARF approach  \npython train.py +experiment=recontab_ssl # ReConTab approach\n</code></pre>"},{"location":"#new-corruption-strategies","title":"\ud83c\udfad New: Corruption Strategies","text":"<p>We've implemented corruption strategies from leading tabular SSL papers:</p> <ul> <li>\ud83c\udfaf VIME - Value imputation and mask estimation (NeurIPS 2020)</li> <li>\ud83c\udf1f SCARF - Contrastive learning with feature corruption (arXiv 2021)</li> <li>\ud83d\udd27 ReConTab - Multi-task reconstruction-based learning</li> </ul>"},{"location":"#sample-data","title":"\ud83c\udfe6 Sample Data","text":"<p>Get started immediately with real transaction data from the IBM TabFormer project - no data preparation needed!</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#tutorials","title":"\ud83d\udcda Tutorials","text":"<p>Learning-oriented guides for newcomers</p> <p>Step-by-step lessons to help you learn Tabular SSL fundamentals. Start here if you're new to the library.</p> <ul> <li>Getting Started - Your first steps with Tabular SSL</li> <li>Basic Usage - Core concepts and workflows</li> <li>Custom Components - Creating your own components</li> </ul>"},{"location":"#how-to-guides","title":"\ud83d\udee0\ufe0f How-to Guides","text":"<p>Problem-oriented solutions for specific tasks</p> <p>Practical guides for accomplishing specific goals and solving real problems.</p> <ul> <li>Data Preparation - Prepare your datasets</li> <li>Model Training - Train models effectively</li> <li>Evaluation - Evaluate model performance</li> <li>Configuring Experiments - Set up experiments</li> </ul>"},{"location":"#reference","title":"\ud83d\udcd6 Reference","text":"<p>Information-oriented technical documentation</p> <p>Complete and accurate technical reference for the library's components and APIs.</p> <ul> <li>API Reference - Complete API documentation</li> <li>Models - Available model components</li> <li>Corruption Strategies - VIME, SCARF, and ReConTab implementations</li> <li>Data - Data handling utilities</li> <li>Configuration - Configuration system</li> <li>Utilities - Helper functions</li> </ul>"},{"location":"#explanation","title":"\ud83d\udca1 Explanation","text":"<p>Understanding-oriented discussions of key topics</p> <p>Background information and conceptual explanations to help you understand the library's design and principles.</p> <ul> <li>Architecture Overview - System design and principles</li> <li>SSL Methods - Self-supervised learning approaches</li> <li>Performance - Optimization and best practices</li> </ul>"},{"location":"#quick-examples","title":"Quick Examples","text":""},{"location":"#demo-scripts","title":"\ud83c\udfad Demo Scripts","text":"<pre><code># Interactive corruption strategies demo\npython demo_corruption_strategies.py\n\n# Real data demo with credit card transactions\npython demo_credit_card_data.py\n</code></pre>"},{"location":"#ssl-experiments","title":"\ud83e\uddea SSL Experiments","text":"<pre><code># VIME: Value imputation + mask estimation\npython train.py +experiment=vime_ssl\n\n# SCARF: Contrastive learning with feature corruption\npython train.py +experiment=scarf_ssl\n\n# ReConTab: Multi-task reconstruction\npython train.py +experiment=recontab_ssl\n</code></pre>"},{"location":"#custom-configuration","title":"\ud83d\udd27 Custom Configuration","text":"<pre><code># Mix and match components\npython train.py model/sequence_encoder=rnn model/event_encoder=mlp\n\n# Use different corruption strategies\npython train.py model/corruption=vime model/corruption.corruption_rate=0.5\n\n# Adjust hyperparameters\npython train.py model.learning_rate=1e-3 data.batch_size=64\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/yourusername/tabular-ssl.git\ncd tabular-ssl\npip install -e .\nexport PYTHONPATH=$PWD/src\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfad State-of-the-Art Corruption Strategies - VIME, SCARF, and ReConTab implementations</li> <li>\ud83c\udfe6 Ready-to-Use Sample Data - IBM TabFormer credit card transaction dataset</li> <li>\ud83e\udde9 Modular Architecture - Mix and match components for custom models</li> <li>\u2699\ufe0f Hydra Configuration - Flexible, hierarchical configuration management</li> <li>\ud83e\uddea Pre-configured SSL Experiments - VIME, SCARF, and ReConTab ready to run</li> <li>\ud83c\udfac Interactive Demos - See corruption strategies in action</li> <li>\ud83d\udcca PyTorch Lightning - Robust training and evaluation framework</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details on how to get involved.</p>"},{"location":"#support","title":"Support","text":"<ul> <li>\ud83d\udcdd GitHub Issues - Bug reports and feature requests</li> <li>\ud83d\udcac Discussions - Questions and community support</li> <li>\ud83d\udce7 Contact - Direct support</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. </p>"},{"location":"explanation/","title":"Explanation","text":"<p>This section provides background information and explanations of the concepts and design decisions behind Tabular SSL.</p>"},{"location":"explanation/#available-topics","title":"Available Topics","text":"<ul> <li>Architecture Overview - System design and components</li> <li>SSL Methods - Self-supervised learning approaches</li> <li>Performance Considerations - Optimization and scaling</li> </ul>"},{"location":"explanation/#key-concepts","title":"Key Concepts","text":""},{"location":"explanation/#self-supervised-learning","title":"Self-Supervised Learning","text":"<p>Self-supervised learning (SSL) is a machine learning paradigm where models learn from unlabeled data by creating their own supervision signals. In the context of tabular data, this involves:</p> <ul> <li>Feature masking and reconstruction</li> <li>Contrastive learning</li> <li>Predictive tasks</li> </ul>"},{"location":"explanation/#architecture","title":"Architecture","text":"<p>The Tabular SSL architecture is designed to:</p> <ul> <li>Handle mixed data types (numerical and categorical)</li> <li>Process variable-length sequences</li> <li>Learn robust representations</li> <li>Scale to large datasets</li> </ul>"},{"location":"explanation/#performance","title":"Performance","text":"<p>Key performance considerations include:</p> <ul> <li>Memory efficiency</li> <li>Training speed</li> <li>Model complexity</li> <li>Inference latency</li> </ul>"},{"location":"explanation/#related-resources","title":"Related Resources","text":"<ul> <li>Tutorials - Step-by-step guides</li> <li>How-to Guides - Practical solutions</li> <li>Reference - Technical documentation </li> </ul>"},{"location":"explanation/architecture/","title":"Architecture Overview","text":"<p>This section explains the architecture and design decisions behind Tabular SSL.</p>"},{"location":"explanation/architecture/#system-design","title":"System Design","text":""},{"location":"explanation/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<p>The Tabular SSL system consists of several key components:</p> <ol> <li>Data Processing Layer</li> <li>Data loading and validation</li> <li>Feature preprocessing</li> <li> <p>Data augmentation</p> </li> <li> <p>Model Layer</p> </li> <li>Component Registry for modular design</li> <li>Feature embedding</li> <li>Encoder components (Transformer, RNN, LSTM, S4, etc.)</li> <li> <p>Task-specific heads</p> </li> <li> <p>Training Layer</p> </li> <li>Self-supervised learning</li> <li>Optimization</li> <li> <p>Monitoring</p> </li> <li> <p>Configuration Layer</p> </li> <li>Hydra configuration management</li> <li>Experiment tracking</li> <li>Parameter validation</li> </ol>"},{"location":"explanation/architecture/#component-registry","title":"Component Registry","text":"<p>One of the core architectural features of Tabular SSL is the Component Registry pattern, which enables a highly modular and extensible design.</p>"},{"location":"explanation/architecture/#registry-design","title":"Registry Design","text":"<p>The Component Registry is a central repository that maps component names to their implementations:</p> <pre><code>class ComponentRegistry:\n    \"\"\"Registry for model components.\"\"\"\n\n    _components: ClassVar[Dict[str, Type['BaseComponent']]] = {}\n\n    @classmethod\n    def register(cls, name: str) -&gt; Type[T]:\n        \"\"\"Register a component class.\"\"\"\n        def decorator(component_cls: Type[T]) -&gt; Type[T]:\n            cls._components[name] = component_cls\n            return component_cls\n        return decorator\n\n    @classmethod\n    def get(cls, name: str) -&gt; Type['BaseComponent']:\n        \"\"\"Get a component class by name.\"\"\"\n        if name not in cls._components:\n            raise KeyError(f\"Component {name} not found in registry\")\n        return cls._components[name]\n</code></pre>"},{"location":"explanation/architecture/#component-configuration","title":"Component Configuration","text":"<p>Each component has its own configuration class that inherits from <code>ComponentConfig</code>:</p> <pre><code>class ComponentConfig(PydanticBaseModel):\n    \"\"\"Base configuration for components.\"\"\"\n\n    name: str = Field(..., description=\"Name of the component\")\n    type: str = Field(..., description=\"Type of the component\")\n\n    @validator('type')\n    def validate_type(cls, v: str) -&gt; str:\n        \"\"\"Validate that the component type exists in the registry.\"\"\"\n        if v not in ComponentRegistry._components:\n            raise ValueError(f\"Component type {v} not found in registry\")\n        return v\n</code></pre>"},{"location":"explanation/architecture/#component-initialization","title":"Component Initialization","text":"<p>Components are initialized using their configuration:</p> <pre><code>def _init_component(self, config: ComponentConfig) -&gt; BaseComponent:\n    \"\"\"Initialize a component from its configuration.\"\"\"\n    component_cls = ComponentRegistry.get(config.type)\n    return component_cls(config)\n</code></pre>"},{"location":"explanation/architecture/#benefits-of-the-registry-pattern","title":"Benefits of the Registry Pattern","text":"<ol> <li>Modularity: Components can be added, removed, or replaced independently</li> <li>Validation: Configuration is validated before components are initialized</li> <li>Extensibility: New components can be added without modifying existing code</li> <li>Dynamic Loading: Components are loaded at runtime based on configuration</li> <li>Type Safety: Component types are checked during initialization</li> </ol>"},{"location":"explanation/architecture/#component-details","title":"Component Details","text":""},{"location":"explanation/architecture/#base-components","title":"Base Components","text":"<p>Tabular SSL defines several base component types:</p> <ol> <li>EventEncoder: Encodes individual events or timesteps</li> <li>SequenceEncoder: Encodes sequences of events</li> <li>EmbeddingLayer: Handles embedding of categorical features</li> <li>ProjectionHead: Projects encoded representations to a different space</li> <li>PredictionHead: Generates predictions from encoded representations</li> </ol> <p>Each component type has multiple implementations that can be selected via configuration.</p>"},{"location":"explanation/architecture/#available-components","title":"Available Components","text":""},{"location":"explanation/architecture/#event-encoders","title":"Event Encoders","text":"<ul> <li><code>mlp_event_encoder</code>: MLP-based event encoder</li> <li><code>autoencoder</code>: Autoencoder-based event encoder</li> <li><code>contrastive</code>: Contrastive learning event encoder</li> </ul>"},{"location":"explanation/architecture/#sequence-encoders","title":"Sequence Encoders","text":"<ul> <li><code>rnn</code>: Basic RNN encoder</li> <li><code>lstm</code>: LSTM encoder</li> <li><code>gru</code>: GRU encoder</li> <li><code>transformer</code>: Transformer encoder</li> <li><code>s4</code>: Diagonal State Space Model (S4) encoder</li> </ul>"},{"location":"explanation/architecture/#embedding-layers","title":"Embedding Layers","text":"<ul> <li><code>categorical_embedding</code>: Embedding layer for categorical variables</li> </ul>"},{"location":"explanation/architecture/#projection-heads","title":"Projection Heads","text":"<ul> <li><code>mlp_projection</code>: MLP-based projection head</li> </ul>"},{"location":"explanation/architecture/#prediction-heads","title":"Prediction Heads","text":"<ul> <li><code>classification</code>: Classification head</li> </ul>"},{"location":"explanation/architecture/#corruption-strategies","title":"Corruption Strategies","text":"<ul> <li><code>random_masking</code>: Random masking corruption</li> <li><code>gaussian_noise</code>: Gaussian noise corruption</li> <li><code>swapping</code>: Feature swapping corruption</li> <li><code>vime</code>: VIME-style corruption</li> <li><code>corruption_pipeline</code>: Pipeline of multiple corruption strategies</li> </ul>"},{"location":"explanation/architecture/#configuration-system","title":"Configuration System","text":"<p>The system uses Hydra's configuration system with structured configuration files:</p> <pre><code>configs/\n\u251c\u2500\u2500 config.yaml                # Main configuration\n\u251c\u2500\u2500 model/                     # Model configurations\n\u2502   \u251c\u2500\u2500 default.yaml          # Default model config\n\u2502   \u251c\u2500\u2500 event_encoder/        # Event encoder configs\n\u2502   \u251c\u2500\u2500 sequence_encoder/     # Sequence encoder configs\n\u2502   \u251c\u2500\u2500 embedding/            # Embedding configs\n\u2502   \u251c\u2500\u2500 projection_head/      # Projection head configs\n\u2502   \u2514\u2500\u2500 prediction_head/      # Prediction head configs\n\u251c\u2500\u2500 data/                     # Data configurations\n\u251c\u2500\u2500 trainer/                  # Training configurations\n\u251c\u2500\u2500 callbacks/                # Callback configurations\n\u251c\u2500\u2500 logger/                   # Logger configurations\n\u251c\u2500\u2500 experiment/               # Experiment configurations\n\u251c\u2500\u2500 hydra/                    # Hydra-specific configurations\n\u2514\u2500\u2500 paths/                    # Path configurations\n</code></pre>"},{"location":"explanation/architecture/#configuration-composition","title":"Configuration Composition","text":"<p>Configurations are composed hierarchically:</p> <pre><code># configs/model/default.yaml\ndefaults:\n  - _self_\n  - event_encoder: mlp.yaml\n  - sequence_encoder: transformer.yaml\n  - embedding: categorical.yaml\n  - projection_head: mlp.yaml\n  - prediction_head: classification.yaml\n\n_target_: tabular_ssl.models.base.BaseModel\n\nmodel:\n  name: tabular_ssl_model\n  type: base\n  event_encoder: ${event_encoder}\n  sequence_encoder: ${sequence_encoder}\n  embedding: ${embedding}\n  projection_head: ${projection_head}\n  prediction_head: ${prediction_head}\n</code></pre>"},{"location":"explanation/architecture/#experiment-configuration","title":"Experiment Configuration","text":"<p>Experiments override specific parts of the configuration:</p> <pre><code># configs/experiment/s4_sequence.yaml\n# @package _global_\n\ndefaults:\n  - override /model/sequence_encoder: s4.yaml\n  - override /trainer: default.yaml\n  - override /model: default.yaml\n  - override /callbacks: default.yaml\n  - _self_\n\ntags: [\"s4\", \"sequence\"]\n\ntrainer:\n  max_epochs: 50\n  gradient_clip_val: 0.5\n\nmodel:\n  optimizer:\n    lr: 5.0e-4\n    weight_decay: 0.05\n</code></pre>"},{"location":"explanation/architecture/#hydra-to-component-integration","title":"Hydra-to-Component Integration","text":"<p>The system translates Hydra configurations to component configurations:</p> <pre><code># Convert Hydra configs to ComponentConfigs\nself.event_encoder_config = ComponentConfig.from_hydra(config.model.event_encoder)\nself.sequence_encoder_config = ComponentConfig.from_hydra(config.model.sequence_encoder)\n\n# Initialize components\nself.event_encoder = self._init_component(self.event_encoder_config)\nself.sequence_encoder = self._init_component(self.sequence_encoder_config)\n</code></pre>"},{"location":"explanation/architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"explanation/architecture/#why-component-registry","title":"Why Component Registry?","text":"<p>The Component Registry pattern was chosen for several reasons:</p> <ol> <li>Separation of Concerns</li> <li>Components focus on their specific functionality</li> <li>Registry handles component discovery and initialization</li> <li> <p>Configuration handles component parameters</p> </li> <li> <p>Extensibility</p> </li> <li>New components can be added without modifying existing code</li> <li>Custom components can be registered by users</li> <li> <p>Experiments can mix and match components</p> </li> <li> <p>Validation</p> </li> <li>Component types are validated during initialization</li> <li>Configuration parameters are validated using Pydantic</li> <li>Better error messages for misconfiguration</li> </ol>"},{"location":"explanation/architecture/#why-hydra-configuration","title":"Why Hydra Configuration?","text":"<p>Hydra provides several benefits for configuration management:</p> <ol> <li>Hierarchical Configuration</li> <li>Configurations are organized into groups</li> <li>Defaults can be overridden selectively</li> <li> <p>Parameters can be composed from multiple sources</p> </li> <li> <p>Command-line Overrides</p> </li> <li>Parameters can be changed at runtime</li> <li>No need to modify configuration files</li> <li> <p>Experiment parameters are explicit</p> </li> <li> <p>Multirun Capabilities</p> </li> <li>Parameter sweeps for experimentation</li> <li>Parallel execution of multiple runs</li> <li>Organized output directories</li> </ol>"},{"location":"explanation/architecture/#implementation-details","title":"Implementation Details","text":""},{"location":"explanation/architecture/#code-organization","title":"Code Organization","text":"<pre><code>src/\n\u251c\u2500\u2500 tabular_ssl/              # Core package\n\u2502   \u251c\u2500\u2500 data/                # Data loading and processing\n\u2502   \u251c\u2500\u2500 models/              # Model implementations\n\u2502   \u2502   \u251c\u2500\u2500 base.py         # Base model and component registry\n\u2502   \u2502   \u251c\u2500\u2500 components.py   # Model components\n\u2502   \u2502   \u2514\u2500\u2500 s4.py           # S4 implementation\n\u2502   \u2514\u2500\u2500 utils/              # Utility functions\n\u2514\u2500\u2500 train.py                 # Training script\n</code></pre>"},{"location":"explanation/architecture/#key-classes","title":"Key Classes","text":""},{"location":"explanation/architecture/#componentregistry","title":"ComponentRegistry","text":"<ul> <li>Central registry for all components</li> <li>Handles component registration and retrieval</li> <li>Ensures type safety</li> </ul>"},{"location":"explanation/architecture/#basecomponent","title":"BaseComponent","text":"<ul> <li>Abstract base class for all components</li> <li>Handles configuration validation</li> <li>Defines common interface</li> </ul>"},{"location":"explanation/architecture/#basemodel","title":"BaseModel","text":"<ul> <li>Main model class</li> <li>Composes components based on configuration</li> <li>Handles training and inference</li> </ul>"},{"location":"explanation/architecture/#componentconfig","title":"ComponentConfig","text":"<ul> <li>Base configuration class</li> <li>Uses Pydantic for validation</li> <li>Integrates with Hydra configuration</li> </ul>"},{"location":"explanation/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"explanation/architecture/#component-design","title":"Component Design","text":"<ol> <li>Lazy Initialization</li> <li>Components are only initialized when needed</li> <li>Configuration is validated early</li> <li> <p>Resources are allocated efficiently</p> </li> <li> <p>Configuration Caching</p> </li> <li>Configurations are parsed once</li> <li>Common configurations are reused</li> <li> <p>Reduces memory overhead</p> </li> <li> <p>Dynamic Component Selection</p> </li> <li>Only required components are initialized</li> <li>Custom components can be more efficient</li> <li>Allows for hardware-specific optimizations</li> </ol>"},{"location":"explanation/architecture/#memory-efficiency","title":"Memory Efficiency","text":"<ol> <li>Batch Processing</li> <li>Dynamic batch sizes</li> <li>Gradient accumulation</li> <li> <p>Memory-efficient attention</p> </li> <li> <p>Model Optimization</p> </li> <li>Parameter sharing</li> <li>Quantization</li> <li>Pruning</li> </ol>"},{"location":"explanation/architecture/#training-speed","title":"Training Speed","text":"<ol> <li>Hardware Acceleration</li> <li>GPU support</li> <li>Mixed precision</li> <li> <p>Parallel processing</p> </li> <li> <p>Optimization</p> </li> <li>Efficient data loading</li> <li>Cached computations</li> <li>Optimized attention</li> </ol>"},{"location":"explanation/architecture/#related-resources","title":"Related Resources","text":"<ul> <li>SSL Methods - Self-supervised learning approaches</li> <li>Performance Considerations - Optimization and scaling</li> <li>API Reference - Technical documentation </li> </ul>"},{"location":"explanation/performance/","title":"Performance Considerations","text":"<p>This section covers performance optimization and scaling considerations for Tabular SSL.</p>"},{"location":"explanation/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"explanation/performance/#batch-processing","title":"Batch Processing","text":"<ol> <li> <p>Dynamic Batch Sizes <pre><code>from tabular_ssl import TabularSSL\n\nmodel = TabularSSL(\n    input_dim=10,\n    batch_size=32,  # Adjust based on available memory\n    gradient_accumulation_steps=4  # Accumulate gradients\n)\n</code></pre></p> </li> <li> <p>Memory-Efficient Attention <pre><code>model = TabularSSL(\n    input_dim=10,\n    attention_type='memory_efficient',  # Use memory-efficient attention\n    chunk_size=64  # Process attention in chunks\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#model-optimization","title":"Model Optimization","text":"<ol> <li> <p>Parameter Sharing <pre><code>model = TabularSSL(\n    input_dim=10,\n    share_parameters=True,  # Share parameters across layers\n    parameter_efficiency=True  # Use parameter-efficient methods\n)\n</code></pre></p> </li> <li> <p>Quantization <pre><code>from tabular_ssl.utils import quantize_model\n\n# Quantize model to reduce memory usage\nquantized_model = quantize_model(\n    model,\n    precision='int8'  # Use 8-bit quantization\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#training-speed","title":"Training Speed","text":""},{"location":"explanation/performance/#hardware-acceleration","title":"Hardware Acceleration","text":"<ol> <li> <p>GPU Support <pre><code>model = TabularSSL(\n    input_dim=10,\n    device='cuda',  # Use GPU\n    mixed_precision=True  # Enable mixed precision training\n)\n</code></pre></p> </li> <li> <p>Multi-GPU Training <pre><code>model = TabularSSL(\n    input_dim=10,\n    distributed=True,  # Enable distributed training\n    num_gpus=4  # Use 4 GPUs\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#optimization-techniques","title":"Optimization Techniques","text":"<ol> <li> <p>Efficient Data Loading <pre><code>from tabular_ssl.data import DataLoader\n\nloader = DataLoader(\n    num_workers=4,  # Use multiple workers\n    pin_memory=True,  # Pin memory for faster transfer\n    prefetch_factor=2  # Prefetch data\n)\n</code></pre></p> </li> <li> <p>Cached Computations <pre><code>model = TabularSSL(\n    input_dim=10,\n    cache_attention=True,  # Cache attention computations\n    cache_size=1000  # Cache size\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"explanation/performance/#data-scaling","title":"Data Scaling","text":"<ol> <li> <p>Large Datasets <pre><code>from tabular_ssl.data import StreamingDataLoader\n\n# Use streaming data loader for large datasets\nloader = StreamingDataLoader(\n    data_path='large_dataset.csv',\n    batch_size=32,\n    chunk_size=10000  # Process data in chunks\n)\n</code></pre></p> </li> <li> <p>Distributed Data Processing <pre><code>from tabular_ssl.data import DistributedDataLoader\n\n# Use distributed data loader\nloader = DistributedDataLoader(\n    data_path='large_dataset.csv',\n    num_workers=4,\n    world_size=4  # Number of processes\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#model-scaling","title":"Model Scaling","text":"<ol> <li> <p>Model Parallelism <pre><code>model = TabularSSL(\n    input_dim=10,\n    model_parallel=True,  # Enable model parallelism\n    num_devices=4  # Split model across 4 devices\n)\n</code></pre></p> </li> <li> <p>Pipeline Parallelism <pre><code>model = TabularSSL(\n    input_dim=10,\n    pipeline_parallel=True,  # Enable pipeline parallelism\n    num_stages=4  # Number of pipeline stages\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"explanation/performance/#metrics","title":"Metrics","text":"<ol> <li> <p>Training Metrics <pre><code>from tabular_ssl.utils import TrainingMonitor\n\nmonitor = TrainingMonitor(\n    metrics=['loss', 'accuracy', 'memory_usage'],\n    log_interval=100\n)\n</code></pre></p> </li> <li> <p>System Metrics <pre><code>from tabular_ssl.utils import SystemMonitor\n\nmonitor = SystemMonitor(\n    metrics=['gpu_usage', 'memory_usage', 'throughput'],\n    log_interval=1\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#profiling","title":"Profiling","text":"<ol> <li> <p>Model Profiling <pre><code>from tabular_ssl.utils import profile_model\n\n# Profile model performance\nprofile = profile_model(\n    model,\n    input_size=(32, 10),  # Batch size, input dimension\n    num_runs=100\n)\n</code></pre></p> </li> <li> <p>Memory Profiling <pre><code>from tabular_ssl.utils import profile_memory\n\n# Profile memory usage\nmemory_profile = profile_memory(\n    model,\n    input_size=(32, 10)\n)\n</code></pre></p> </li> </ol>"},{"location":"explanation/performance/#best-practices","title":"Best Practices","text":""},{"location":"explanation/performance/#memory-management","title":"Memory Management","text":"<ol> <li>Batch Size Selection</li> <li>Start with small batch sizes</li> <li>Gradually increase if memory allows</li> <li> <p>Use gradient accumulation for large batches</p> </li> <li> <p>Model Architecture</p> </li> <li>Use parameter-efficient architectures</li> <li>Implement memory-efficient attention</li> <li>Consider model quantization</li> </ol>"},{"location":"explanation/performance/#training-optimization","title":"Training Optimization","text":"<ol> <li>Hardware Utilization</li> <li>Use GPU acceleration</li> <li>Enable mixed precision training</li> <li> <p>Implement distributed training</p> </li> <li> <p>Data Processing</p> </li> <li>Use efficient data loaders</li> <li>Implement data prefetching</li> <li>Cache frequent computations</li> </ol>"},{"location":"explanation/performance/#related-resources","title":"Related Resources","text":"<ul> <li>Architecture Overview - System design details</li> <li>SSL Methods - Learning approaches</li> <li>API Reference - Technical documentation </li> </ul>"},{"location":"explanation/ssl-methods/","title":"Self-Supervised Learning Methods","text":"<p>This section explains the self-supervised learning methods implemented in Tabular SSL.</p>"},{"location":"explanation/ssl-methods/#overview","title":"Overview","text":"<p>Self-supervised learning (SSL) is a machine learning paradigm where models learn from unlabeled data by creating their own supervision signals. In Tabular SSL, we implement several SSL methods:</p> <ol> <li>Masked Feature Prediction</li> <li>Contrastive Learning</li> <li>Feature Reconstruction</li> </ol>"},{"location":"explanation/ssl-methods/#masked-feature-prediction","title":"Masked Feature Prediction","text":""},{"location":"explanation/ssl-methods/#how-it-works","title":"How It Works","text":"<ol> <li>Feature Masking</li> <li>Randomly mask a portion of features</li> <li>Use a masking ratio (default: 0.15)</li> <li> <p>Preserve feature relationships</p> </li> <li> <p>Prediction Task</p> </li> <li>Predict masked features</li> <li>Use surrounding features as context</li> <li>Learn feature dependencies</li> </ol>"},{"location":"explanation/ssl-methods/#implementation","title":"Implementation","text":"<pre><code>from tabular_ssl import TabularSSL\n\nmodel = TabularSSL(\n    input_dim=10,\n    mask_ratio=0.15  # 15% of features masked\n)\n\n# Train with masked feature prediction\nhistory = model.train(\n    data=train_data,\n    ssl_method='masked_prediction'\n)\n</code></pre>"},{"location":"explanation/ssl-methods/#contrastive-learning","title":"Contrastive Learning","text":""},{"location":"explanation/ssl-methods/#how-it-works_1","title":"How It Works","text":"<ol> <li>Data Augmentation</li> <li>Create positive pairs</li> <li>Apply transformations</li> <li> <p>Generate negative samples</p> </li> <li> <p>Contrastive Loss</p> </li> <li>Maximize similarity of positive pairs</li> <li>Minimize similarity of negative pairs</li> <li>Learn robust representations</li> </ol>"},{"location":"explanation/ssl-methods/#implementation_1","title":"Implementation","text":"<pre><code>from tabular_ssl import TabularSSL\n\nmodel = TabularSSL(\n    input_dim=10,\n    ssl_method='contrastive'\n)\n\n# Train with contrastive learning\nhistory = model.train(\n    data=train_data,\n    temperature=0.07,  # Temperature parameter\n    queue_size=65536   # Size of memory queue\n)\n</code></pre>"},{"location":"explanation/ssl-methods/#feature-reconstruction","title":"Feature Reconstruction","text":""},{"location":"explanation/ssl-methods/#how-it-works_2","title":"How It Works","text":"<ol> <li>Autoencoder Architecture</li> <li>Encode input features</li> <li>Decode to reconstruct</li> <li> <p>Learn feature representations</p> </li> <li> <p>Reconstruction Loss</p> </li> <li>Minimize reconstruction error</li> <li>Learn feature relationships</li> <li>Capture data structure</li> </ol>"},{"location":"explanation/ssl-methods/#implementation_2","title":"Implementation","text":"<pre><code>from tabular_ssl import TabularSSL\n\nmodel = TabularSSL(\n    input_dim=10,\n    ssl_method='reconstruction'\n)\n\n# Train with feature reconstruction\nhistory = model.train(\n    data=train_data,\n    reconstruction_weight=1.0\n)\n</code></pre>"},{"location":"explanation/ssl-methods/#combining-methods","title":"Combining Methods","text":""},{"location":"explanation/ssl-methods/#multi-task-learning","title":"Multi-Task Learning","text":"<pre><code>from tabular_ssl import TabularSSL\n\nmodel = TabularSSL(\n    input_dim=10,\n    ssl_methods=['masked_prediction', 'contrastive']\n)\n\n# Train with multiple SSL methods\nhistory = model.train(\n    data=train_data,\n    method_weights={\n        'masked_prediction': 0.5,\n        'contrastive': 0.5\n    }\n)\n</code></pre>"},{"location":"explanation/ssl-methods/#method-selection","title":"Method Selection","text":""},{"location":"explanation/ssl-methods/#when-to-use-each-method","title":"When to Use Each Method","text":"<ol> <li>Masked Feature Prediction</li> <li>When feature relationships are important</li> <li>For structured tabular data</li> <li> <p>When interpretability is needed</p> </li> <li> <p>Contrastive Learning</p> </li> <li>For robust representations</li> <li>When data augmentation is possible</li> <li> <p>For transfer learning</p> </li> <li> <p>Feature Reconstruction</p> </li> <li>For simple feature learning</li> <li>When computational efficiency is important</li> <li>For basic representation learning</li> </ol>"},{"location":"explanation/ssl-methods/#best-practices","title":"Best Practices","text":""},{"location":"explanation/ssl-methods/#method-selection_1","title":"Method Selection","text":"<ol> <li>Data Characteristics</li> <li>Consider data structure</li> <li>Evaluate feature relationships</li> <li> <p>Assess data quality</p> </li> <li> <p>Task Requirements</p> </li> <li>Define learning objectives</li> <li>Consider downstream tasks</li> <li> <p>Evaluate computational needs</p> </li> <li> <p>Resource Constraints</p> </li> <li>Consider memory usage</li> <li>Evaluate training time</li> <li>Assess hardware requirements</li> </ol>"},{"location":"explanation/ssl-methods/#implementation-tips","title":"Implementation Tips","text":"<ol> <li>Hyperparameter Tuning</li> <li>Masking ratio</li> <li>Temperature parameter</li> <li> <p>Loss weights</p> </li> <li> <p>Training Strategy</p> </li> <li>Learning rate scheduling</li> <li>Batch size selection</li> <li> <p>Early stopping</p> </li> <li> <p>Evaluation</p> </li> <li>Monitor SSL metrics</li> <li>Evaluate downstream performance</li> <li>Compare methods</li> </ol>"},{"location":"explanation/ssl-methods/#related-resources","title":"Related Resources","text":"<ul> <li>Architecture Overview - System design details</li> <li>Performance Considerations - Optimization guide</li> <li>API Reference - Technical documentation </li> </ul>"},{"location":"how-to-guides/","title":"How-to Guides","text":"<p>This section contains practical guides for common tasks and specific use cases with Tabular SSL.</p>"},{"location":"how-to-guides/#available-guides","title":"Available Guides","text":"<ul> <li>Data Preparation - Learn how to prepare your data for Tabular SSL</li> <li>Model Training - Best practices for training models</li> <li>Evaluation - How to evaluate and interpret model results</li> </ul>"},{"location":"how-to-guides/#common-tasks","title":"Common Tasks","text":""},{"location":"how-to-guides/#data-preparation","title":"Data Preparation","text":"<ul> <li>Loading and preprocessing data</li> <li>Handling categorical variables</li> <li>Feature scaling and normalization</li> <li>Dealing with missing values</li> </ul>"},{"location":"how-to-guides/#model-training","title":"Model Training","text":"<ul> <li>Setting up training configurations</li> <li>Monitoring training progress</li> <li>Saving and loading models</li> <li>Hyperparameter tuning</li> </ul>"},{"location":"how-to-guides/#evaluation","title":"Evaluation","text":"<ul> <li>Computing performance metrics</li> <li>Visualizing results</li> <li>Model interpretation</li> <li>Error analysis</li> </ul>"},{"location":"how-to-guides/#related-resources","title":"Related Resources","text":"<ul> <li>API Reference - Detailed API documentation</li> <li>Tutorials - Step-by-step learning guides</li> <li>Explanation - Background information </li> </ul>"},{"location":"how-to-guides/configuring-experiments/","title":"Configuring Experiments","text":"<p>This guide explains how to configure and run experiments using Hydra in Tabular SSL.</p>"},{"location":"how-to-guides/configuring-experiments/#introduction","title":"Introduction","text":"<p>Tabular SSL uses Hydra for configuration management, which enables hierarchical configuration composition, command-line overrides, and experiment tracking. This guide will show you how to:</p> <ol> <li>Use the configuration structure</li> <li>Create and run experiments</li> <li>Override default parameters</li> <li>Run parameter sweeps</li> </ol>"},{"location":"how-to-guides/configuring-experiments/#configuration-structure","title":"Configuration Structure","text":"<p>The configuration files are organized in a hierarchical structure:</p> <pre><code>configs/\n\u251c\u2500\u2500 config.yaml                # Main configuration\n\u251c\u2500\u2500 model/                     # Model configurations\n\u2502   \u251c\u2500\u2500 default.yaml          # Default model config\n\u2502   \u251c\u2500\u2500 event_encoder/        # Event encoder configs\n\u2502   \u251c\u2500\u2500 sequence_encoder/     # Sequence encoder configs\n\u2502   \u251c\u2500\u2500 embedding/            # Embedding configs\n\u2502   \u251c\u2500\u2500 projection_head/      # Projection head configs\n\u2502   \u2514\u2500\u2500 prediction_head/      # Prediction head configs\n\u251c\u2500\u2500 data/                     # Data configurations\n\u251c\u2500\u2500 trainer/                  # Training configurations\n\u251c\u2500\u2500 callbacks/                # Callback configurations\n\u251c\u2500\u2500 logger/                   # Logger configurations\n\u251c\u2500\u2500 experiment/               # Experiment configurations\n\u251c\u2500\u2500 hydra/                    # Hydra-specific configurations\n\u2514\u2500\u2500 paths/                    # Path configurations\n</code></pre>"},{"location":"how-to-guides/configuring-experiments/#basic-usage","title":"Basic Usage","text":""},{"location":"how-to-guides/configuring-experiments/#running-with-default-configuration","title":"Running with Default Configuration","text":"<p>To run with the default configuration:</p> <pre><code>python src/train.py\n</code></pre> <p>This will use the configuration in <code>configs/config.yaml</code>, which composes configurations from the other directories.</p>"},{"location":"how-to-guides/configuring-experiments/#overriding-parameters","title":"Overriding Parameters","text":"<p>You can override any parameter using the command line:</p> <pre><code>python src/train.py model.optimizer.lr=0.001 trainer.max_epochs=50\n</code></pre> <p>This will override the learning rate and the maximum number of epochs while using the default values for all other parameters.</p>"},{"location":"how-to-guides/configuring-experiments/#using-a-specific-configuration","title":"Using a Specific Configuration","text":"<p>You can use a specific configuration for a component:</p> <pre><code>python src/train.py model/event_encoder=mlp model/sequence_encoder=transformer\n</code></pre> <p>This will use the MLP event encoder and Transformer sequence encoder configurations.</p>"},{"location":"how-to-guides/configuring-experiments/#creating-experiments","title":"Creating Experiments","text":""},{"location":"how-to-guides/configuring-experiments/#experiment-configuration-files","title":"Experiment Configuration Files","text":"<p>Experiment configuration files are stored in <code>configs/experiment/</code> and provide a way to group parameter overrides.</p> <p>Here's an example experiment configuration file:</p> <pre><code># configs/experiment/transformer_ssl.yaml\n# @package _global_\n\ndefaults:\n  - override /model/event_encoder: mlp.yaml\n  - override /model/sequence_encoder: transformer.yaml\n  - override /trainer: default.yaml\n  - override /model: default.yaml\n  - override /callbacks: default.yaml\n  - _self_\n\ntags: [\"transformer\", \"ssl\"]\n\nseed: 12345\n\ntrainer:\n  max_epochs: 100\n  gradient_clip_val: 0.5\n\nmodel:\n  optimizer:\n    lr: 1.0e-4\n    weight_decay: 0.01\n</code></pre> <p>Key things to note:</p> <ol> <li><code># @package _global_</code>: This indicates that the configuration should be merged at the global level</li> <li><code>defaults</code>: Specifies which configurations to use as defaults</li> <li><code>override /path/to/config</code>: Overrides a specific configuration</li> <li><code>_self_</code>: Ensures that the current file's configurations are applied after all others</li> </ol>"},{"location":"how-to-guides/configuring-experiments/#running-an-experiment","title":"Running an Experiment","text":"<p>To run an experiment:</p> <pre><code>python src/train.py experiment=transformer_ssl\n</code></pre> <p>This will use the configuration defined in <code>configs/experiment/transformer_ssl.yaml</code>.</p>"},{"location":"how-to-guides/configuring-experiments/#extending-an-experiment","title":"Extending an Experiment","text":"<p>You can extend an experiment by overriding its parameters:</p> <pre><code>python src/train.py experiment=transformer_ssl trainer.max_epochs=200\n</code></pre> <p>This will use the transformer_ssl experiment configuration with the maximum epochs set to 200.</p>"},{"location":"how-to-guides/configuring-experiments/#debugging","title":"Debugging","text":""},{"location":"how-to-guides/configuring-experiments/#debug-mode","title":"Debug Mode","text":"<p>You can run in debug mode to speed up debugging:</p> <pre><code>python src/train.py debug=true\n</code></pre> <p>This will typically: - Run on a smaller dataset - Use fewer epochs - Disable certain features like logging</p>"},{"location":"how-to-guides/configuring-experiments/#experiment-tracking","title":"Experiment Tracking","text":""},{"location":"how-to-guides/configuring-experiments/#logging-and-output","title":"Logging and Output","text":"<p>When you run an experiment, Hydra creates an output directory for that run:</p> <pre><code>outputs/\n\u2514\u2500\u2500 2023-06-15/\n    \u2514\u2500\u2500 12-34-56/\n        \u251c\u2500\u2500 .hydra/\n        \u2502   \u251c\u2500\u2500 config.yaml\n        \u2502   \u251c\u2500\u2500 hydra.yaml\n        \u2502   \u2514\u2500\u2500 overrides.yaml\n        \u251c\u2500\u2500 checkpoints/\n        \u2514\u2500\u2500 logs/\n</code></pre> <p>The <code>.hydra/</code> directory contains the full configuration that was used for the run.</p>"},{"location":"how-to-guides/configuring-experiments/#tags","title":"Tags","text":"<p>You can add tags to your experiments:</p> <pre><code># configs/experiment/transformer_ssl.yaml\ntags: [\"transformer\", \"ssl\"]\n</code></pre> <p>Or via the command line:</p> <pre><code>python src/train.py tags=\"[transformer, ssl]\"\n</code></pre> <p>These tags can be used for filtering and grouping experiments.</p>"},{"location":"how-to-guides/configuring-experiments/#parameter-sweeps","title":"Parameter Sweeps","text":"<p>Hydra allows you to perform parameter sweeps by specifying multiple values for a parameter.</p>"},{"location":"how-to-guides/configuring-experiments/#basic-sweep","title":"Basic Sweep","text":"<pre><code>python src/train.py -m model.optimizer.lr=1e-3,1e-4,1e-5\n</code></pre> <p>This will run three experiments with different learning rates.</p>"},{"location":"how-to-guides/configuring-experiments/#multi-parameter-sweep","title":"Multi-Parameter Sweep","text":"<pre><code>python src/train.py -m model.optimizer.lr=1e-3,1e-4 model.optimizer.weight_decay=0.01,0.001\n</code></pre> <p>This will run 4 experiments (2 learning rates \u00d7 2 weight decay values).</p>"},{"location":"how-to-guides/configuring-experiments/#sweep-with-experiment","title":"Sweep with Experiment","text":"<pre><code>python src/train.py -m experiment=transformer_ssl,s4_ssl\n</code></pre> <p>This will run both the transformer_ssl and s4_ssl experiments.</p>"},{"location":"how-to-guides/configuring-experiments/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"how-to-guides/configuring-experiments/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can use environment variables in your configurations:</p> <pre><code>data:\n  path: ${oc.env:DATA_PATH,/default/path}\n</code></pre> <p>This will use the <code>DATA_PATH</code> environment variable if it exists, or fall back to <code>/default/path</code>.</p>"},{"location":"how-to-guides/configuring-experiments/#using-interpolation","title":"Using Interpolation","text":"<p>You can reference other configuration values:</p> <pre><code>model:\n  input_dim: 64\n  hidden_dim: ${model.input_dim}  # References input_dim\n</code></pre>"},{"location":"how-to-guides/configuring-experiments/#dynamic-default-values","title":"Dynamic Default Values","text":"<p>You can compute default values based on other parameters:</p> <pre><code>model:\n  input_dim: 64\n  hidden_dim: ${eval:2 * ${model.input_dim}}  # Dynamic computation\n</code></pre>"},{"location":"how-to-guides/configuring-experiments/#best-practices","title":"Best Practices","text":""},{"location":"how-to-guides/configuring-experiments/#naming-conventions","title":"Naming Conventions","text":"<ol> <li>Use descriptive names for experiment files</li> <li>Group related parameters together</li> <li>Use consistent naming across configurations</li> </ol>"},{"location":"how-to-guides/configuring-experiments/#configuration-structure_1","title":"Configuration Structure","text":"<ol> <li>Keep configuration files small and focused</li> <li>Use defaults for common parameters</li> <li>Override only what's necessary</li> </ol>"},{"location":"how-to-guides/configuring-experiments/#experiment-management","title":"Experiment Management","text":"<ol> <li>Use meaningful tags for experiments</li> <li>Add a brief description in the experiment file</li> <li>Document key parameter choices</li> </ol>"},{"location":"how-to-guides/configuring-experiments/#typical-workflow","title":"Typical Workflow","text":"<ol> <li>Start with an existing experiment: <code>python src/train.py experiment=transformer_ssl</code></li> <li>Make modifications via the command line: <code>python src/train.py experiment=transformer_ssl model.optimizer.lr=1e-5</code></li> <li>If the modifications work well, create a new experiment file</li> <li>Run parameter sweeps to find optimal values: <code>python src/train.py -m experiment=my_new_experiment model.optimizer.lr=1e-3,1e-4,1e-5</code></li> </ol>"},{"location":"how-to-guides/configuring-experiments/#conclusion","title":"Conclusion","text":"<p>Hydra provides a powerful way to configure and track experiments in Tabular SSL. By using experiment configuration files, command-line overrides, and parameter sweeps, you can efficiently explore the parameter space and find the best configurations for your specific task. </p>"},{"location":"how-to-guides/data-preparation/","title":"Data Preparation Guide","text":"<p>This guide covers best practices for preparing your data for use with Tabular SSL.</p>"},{"location":"how-to-guides/data-preparation/#loading-data","title":"Loading Data","text":""},{"location":"how-to-guides/data-preparation/#from-csv-files","title":"From CSV Files","text":"<pre><code>from tabular_ssl.data import DataLoader\n\n# Initialize the data loader\ndata_loader = DataLoader()\n\n# Load data from CSV\ndata = data_loader.load_data('path/to/your/data.csv')\n</code></pre>"},{"location":"how-to-guides/data-preparation/#from-pandas-dataframe","title":"From Pandas DataFrame","text":"<pre><code>import pandas as pd\n\n# Create or load your DataFrame\ndf = pd.DataFrame({\n    'numeric_col': [1, 2, 3],\n    'categorical_col': ['A', 'B', 'A']\n})\n\n# Use the data loader\ndata_loader = DataLoader()\nprocessed_data = data_loader.preprocess(df)\n</code></pre>"},{"location":"how-to-guides/data-preparation/#handling-different-data-types","title":"Handling Different Data Types","text":""},{"location":"how-to-guides/data-preparation/#categorical-variables","title":"Categorical Variables","text":"<pre><code># Specify categorical columns\ncategorical_cols = ['category1', 'category2']\nprocessed_data = data_loader.preprocess(\n    data,\n    categorical_cols=categorical_cols\n)\n</code></pre>"},{"location":"how-to-guides/data-preparation/#numerical-variables","title":"Numerical Variables","text":"<pre><code># Numerical columns are automatically detected\n# You can specify scaling options\nprocessed_data = data_loader.preprocess(\n    data,\n    scale_numerical=True,  # Enable scaling\n    scaler='standard'      # Use standard scaler\n)\n</code></pre>"},{"location":"how-to-guides/data-preparation/#dealing-with-missing-values","title":"Dealing with Missing Values","text":""},{"location":"how-to-guides/data-preparation/#automatic-handling","title":"Automatic Handling","text":"<pre><code># The data loader automatically handles missing values\nprocessed_data = data_loader.preprocess(\n    data,\n    handle_missing=True,  # Enable missing value handling\n    missing_strategy='mean'  # Use mean imputation\n)\n</code></pre>"},{"location":"how-to-guides/data-preparation/#manual-handling","title":"Manual Handling","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Fill missing values\ndata = data.fillna({\n    'numeric_col': data['numeric_col'].mean(),\n    'categorical_col': data['categorical_col'].mode()[0]\n})\n</code></pre>"},{"location":"how-to-guides/data-preparation/#feature-engineering","title":"Feature Engineering","text":""},{"location":"how-to-guides/data-preparation/#creating-new-features","title":"Creating New Features","text":"<pre><code># Add interaction terms\ndata['interaction'] = data['feature1'] * data['feature2']\n\n# Add polynomial features\ndata['feature1_squared'] = data['feature1'] ** 2\n</code></pre>"},{"location":"how-to-guides/data-preparation/#feature-selection","title":"Feature Selection","text":"<pre><code>from tabular_ssl.utils import select_features\n\n# Select features based on importance\nselected_features = select_features(\n    data,\n    target_col='target',\n    method='importance',\n    threshold=0.01\n)\n</code></pre>"},{"location":"how-to-guides/data-preparation/#data-validation","title":"Data Validation","text":""},{"location":"how-to-guides/data-preparation/#checking-data-quality","title":"Checking Data Quality","text":"<pre><code>from tabular_ssl.utils import validate_data\n\n# Validate data before processing\nvalidation_results = validate_data(data)\nprint(validation_results)\n</code></pre>"},{"location":"how-to-guides/data-preparation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>Inconsistent Data Types <pre><code># Convert columns to correct types\ndata['numeric_col'] = pd.to_numeric(data['numeric_col'])\ndata['categorical_col'] = data['categorical_col'].astype('category')\n</code></pre></p> </li> <li> <p>Outliers <pre><code># Remove outliers\ndata = data[data['numeric_col'].between(\n    data['numeric_col'].quantile(0.01),\n    data['numeric_col'].quantile(0.99)\n)]\n</code></pre></p> </li> </ol>"},{"location":"how-to-guides/data-preparation/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate your data before processing</li> <li>Handle missing values appropriately for your use case</li> <li>Scale numerical features when necessary</li> <li>Encode categorical variables properly</li> <li>Check for and handle outliers</li> <li>Document your preprocessing steps</li> </ol>"},{"location":"how-to-guides/data-preparation/#related-resources","title":"Related Resources","text":"<ul> <li>Model Training - Next steps after data preparation</li> <li>API Reference - Detailed API documentation</li> <li>Tutorials - Step-by-step guides </li> </ul>"},{"location":"how-to-guides/evaluation/","title":"Model Evaluation Guide","text":"<p>This guide covers how to evaluate and interpret your Tabular SSL models.</p>"},{"location":"how-to-guides/evaluation/#basic-evaluation","title":"Basic Evaluation","text":""},{"location":"how-to-guides/evaluation/#computing-metrics","title":"Computing Metrics","text":"<pre><code>from tabular_ssl.utils import evaluate_model\n\n# Evaluate model performance\nmetrics = evaluate_model(\n    model,\n    test_data,\n    metrics=['accuracy', 'f1', 'precision', 'recall']\n)\nprint(metrics)\n</code></pre>"},{"location":"how-to-guides/evaluation/#cross-validation","title":"Cross-Validation","text":"<pre><code>from tabular_ssl.utils import cross_validate\n\n# Perform k-fold cross-validation\ncv_results = cross_validate(\n    model,\n    data,\n    n_splits=5,\n    metrics=['accuracy', 'f1']\n)\nprint(cv_results)\n</code></pre>"},{"location":"how-to-guides/evaluation/#advanced-evaluation","title":"Advanced Evaluation","text":""},{"location":"how-to-guides/evaluation/#custom-metrics","title":"Custom Metrics","text":"<pre><code>from tabular_ssl.utils import CustomMetric\n\n# Define custom metric\ndef custom_metric(y_true, y_pred):\n    # Your custom metric implementation\n    return score\n\n# Evaluate with custom metric\nmetrics = evaluate_model(\n    model,\n    test_data,\n    metrics=['accuracy', CustomMetric(custom_metric)]\n)\n</code></pre>"},{"location":"how-to-guides/evaluation/#model-comparison","title":"Model Comparison","text":"<pre><code>from tabular_ssl.utils import compare_models\n\n# Compare multiple models\ncomparison = compare_models(\n    models=[model1, model2, model3],\n    test_data=test_data,\n    metrics=['accuracy', 'f1']\n)\nprint(comparison)\n</code></pre>"},{"location":"how-to-guides/evaluation/#visualization","title":"Visualization","text":""},{"location":"how-to-guides/evaluation/#training-history","title":"Training History","text":"<pre><code>from tabular_ssl.utils import plot_training_history\n\n# Plot training metrics\nfig = plot_training_history(history)\nfig.show()\n</code></pre>"},{"location":"how-to-guides/evaluation/#performance-plots","title":"Performance Plots","text":"<pre><code>from tabular_ssl.utils import plot_performance\n\n# Plot various performance metrics\nfig = plot_performance(\n    model,\n    test_data,\n    plot_types=['confusion_matrix', 'roc_curve', 'precision_recall']\n)\nfig.show()\n</code></pre>"},{"location":"how-to-guides/evaluation/#model-interpretation","title":"Model Interpretation","text":""},{"location":"how-to-guides/evaluation/#feature-importance","title":"Feature Importance","text":"<pre><code>from tabular_ssl.utils import get_feature_importance\n\n# Get feature importance scores\nimportance = get_feature_importance(model, test_data)\nprint(importance)\n</code></pre>"},{"location":"how-to-guides/evaluation/#shap-values","title":"SHAP Values","text":"<pre><code>from tabular_ssl.utils import get_shap_values\n\n# Compute SHAP values\nshap_values = get_shap_values(model, test_data)\n\n# Plot SHAP summary\nplot_shap_summary(shap_values, test_data)\n</code></pre>"},{"location":"how-to-guides/evaluation/#error-analysis","title":"Error Analysis","text":""},{"location":"how-to-guides/evaluation/#error-distribution","title":"Error Distribution","text":"<pre><code>from tabular_ssl.utils import analyze_errors\n\n# Analyze prediction errors\nerror_analysis = analyze_errors(\n    model,\n    test_data,\n    analysis_types=['distribution', 'correlation']\n)\nprint(error_analysis)\n</code></pre>"},{"location":"how-to-guides/evaluation/#error-visualization","title":"Error Visualization","text":"<pre><code>from tabular_ssl.utils import plot_errors\n\n# Plot error analysis\nfig = plot_errors(\n    model,\n    test_data,\n    plot_types=['residuals', 'error_distribution']\n)\nfig.show()\n</code></pre>"},{"location":"how-to-guides/evaluation/#best-practices","title":"Best Practices","text":"<ol> <li>Use multiple evaluation metrics</li> <li>Perform cross-validation for robust results</li> <li>Compare against baseline models</li> <li>Analyze error patterns</li> <li>Visualize results for better understanding</li> <li>Consider domain-specific metrics</li> <li>Document evaluation methodology</li> <li>Validate results with statistical tests</li> </ol>"},{"location":"how-to-guides/evaluation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"how-to-guides/evaluation/#unbalanced-data","title":"Unbalanced Data","text":"<pre><code>from tabular_ssl.utils import balanced_metrics\n\n# Use balanced metrics\nmetrics = evaluate_model(\n    model,\n    test_data,\n    metrics=['balanced_accuracy', 'f1']\n)\n</code></pre>"},{"location":"how-to-guides/evaluation/#small-test-sets","title":"Small Test Sets","text":"<pre><code># Use bootstrapping for small test sets\nfrom tabular_ssl.utils import bootstrap_evaluation\n\nresults = bootstrap_evaluation(\n    model,\n    test_data,\n    n_bootstrap=1000,\n    metrics=['accuracy', 'f1']\n)\n</code></pre>"},{"location":"how-to-guides/evaluation/#related-resources","title":"Related Resources","text":"<ul> <li>Model Training - Training your model</li> <li>Data Preparation - Preparing your data</li> <li>API Reference - Detailed API documentation </li> </ul>"},{"location":"how-to-guides/model-training/","title":"How to Train Models Effectively","text":"<p>This guide solves specific problems you might encounter when training Tabular SSL models. Each section addresses a common training challenge with practical solutions.</p>"},{"location":"how-to-guides/model-training/#problem-your-first-training-run","title":"Problem: Your First Training Run","text":"<p>Goal: Get a model training successfully from scratch</p> <p>Solution: Use pre-configured experiments</p> <pre><code># Start with the simplest baseline\npython train.py +experiment=simple_mlp\n\n# Check outputs directory for results\nls outputs/$(date +%Y-%m-%d)/\n</code></pre> <p>Why this works: Pre-configured experiments have tested hyperparameters and compatible component combinations.</p>"},{"location":"how-to-guides/model-training/#problem-training-takes-too-long","title":"Problem: Training Takes Too Long","text":"<p>Goal: Speed up training without sacrificing too much performance</p>"},{"location":"how-to-guides/model-training/#solution-1-reduce-model-complexity","title":"Solution 1: Reduce Model Complexity","text":"<pre><code># Use smaller model components\npython train.py model/sequence_encoder=null data.batch_size=128\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-use-mixed-precision-training","title":"Solution 2: Use Mixed Precision Training","text":"<pre><code># Enable 16-bit precision for faster training\npython train.py +experiment=simple_mlp trainer.precision=16-mixed\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-optimize-data-loading","title":"Solution 3: Optimize Data Loading","text":"<pre><code># Increase data workers and enable memory pinning\npython train.py data.num_workers=8 data.pin_memory=true\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-running-out-of-gpu-memory","title":"Problem: Running Out of GPU Memory","text":"<p>Goal: Fit your model in available GPU memory</p>"},{"location":"how-to-guides/model-training/#solution-1-reduce-batch-size","title":"Solution 1: Reduce Batch Size","text":"<pre><code># Halve the batch size\npython train.py +experiment=transformer_small data.batch_size=32\n\n# Use gradient accumulation to maintain effective batch size\npython train.py data.batch_size=32 trainer.accumulate_grad_batches=2\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-use-smaller-models","title":"Solution 2: Use Smaller Models","text":"<pre><code># Switch to more memory-efficient sequence encoder\npython train.py model/sequence_encoder=rnn\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-enable-gradient-checkpointing","title":"Solution 3: Enable Gradient Checkpointing","text":"<pre><code># configs/experiments/memory_efficient.yaml\n# @package _global_\ndefaults:\n  - override /model/sequence_encoder: transformer\n\nmodel:\n  sequence_encoder:\n    enable_checkpointing: true\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-model-isnt-learning-loss-not-decreasing","title":"Problem: Model Isn't Learning (Loss Not Decreasing)","text":"<p>Goal: Debug and fix training issues</p>"},{"location":"how-to-guides/model-training/#solution-1-check-learning-rate","title":"Solution 1: Check Learning Rate","text":"<pre><code># Try different learning rates\npython train.py +experiment=simple_mlp model.learning_rate=1e-2  # Higher\npython train.py +experiment=simple_mlp model.learning_rate=1e-5  # Lower\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-verify-data-loading","title":"Solution 2: Verify Data Loading","text":"<pre><code># Use simple data config to test\npython train.py +experiment=simple_mlp data=simple\n\n# Enable debug mode to see data shapes\npython train.py +experiment=simple_mlp debug=true\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-add-gradient-clipping","title":"Solution 3: Add Gradient Clipping","text":"<pre><code># Prevent exploding gradients\npython train.py +experiment=simple_mlp trainer.gradient_clip_val=1.0\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-training-stops-early-due-to-errors","title":"Problem: Training Stops Early Due to Errors","text":"<p>Goal: Resolve common training errors</p>"},{"location":"how-to-guides/model-training/#solution-1-dimension-mismatches","title":"Solution 1: Dimension Mismatches","text":"<pre><code># Check component compatibility in config files\npython train.py --config-name=config +experiment=simple_mlp --print-config\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-cuda-errors","title":"Solution 2: CUDA Errors","text":"<pre><code># Force CPU training for debugging\npython train.py +experiment=simple_mlp trainer.accelerator=cpu\n\n# Or specify GPU explicitly\npython train.py +experiment=simple_mlp trainer.devices=1\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-need-to-monitor-training-progress","title":"Problem: Need to Monitor Training Progress","text":"<p>Goal: Track training metrics and visualize progress</p>"},{"location":"how-to-guides/model-training/#solution-1-enable-logging","title":"Solution 1: Enable Logging","text":"<pre><code># Use Weights &amp; Biases\npython train.py +experiment=simple_mlp logger=wandb\n\n# Use CSV logging for local development\npython train.py +experiment=simple_mlp logger=csv\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-add-callbacks","title":"Solution 2: Add Callbacks","text":"<pre><code># configs/experiments/monitored_training.yaml\ndefaults:\n  - /callbacks: default\n\ncallbacks:\n  model_checkpoint:\n    monitor: \"val/loss\"\n    save_top_k: 3\n  early_stopping:\n    monitor: \"val/loss\"\n    patience: 10\n    min_delta: 0.001\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-custom-progress-logging","title":"Solution 3: Custom Progress Logging","text":"<pre><code># Increase logging frequency\npython train.py +experiment=simple_mlp trainer.log_every_n_steps=10\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-hyperparameter-tuning","title":"Problem: Hyperparameter Tuning","text":"<p>Goal: Find optimal hyperparameters systematically</p>"},{"location":"how-to-guides/model-training/#solution-1-manual-grid-search","title":"Solution 1: Manual Grid Search","text":"<pre><code># Test different learning rates\nfor lr in 1e-2 1e-3 1e-4; do\n    python train.py +experiment=simple_mlp model.learning_rate=$lr\ndone\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-use-hydra-multirun","title":"Solution 2: Use Hydra Multirun","text":"<pre><code># Test multiple hyperparameters simultaneously\npython train.py -m +experiment=simple_mlp \\\n    model.learning_rate=1e-2,1e-3,1e-4 \\\n    data.batch_size=32,64,128\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-structured-experiment-sweeps","title":"Solution 3: Structured Experiment Sweeps","text":"<pre><code># configs/experiments/sweep_transformer.yaml\n# @package _global_\ndefaults:\n  - override /model/sequence_encoder: transformer\n\n# Hydra sweep configuration\nhydra:\n  mode: MULTIRUN\n  sweeper:\n    params:\n      model.learning_rate: 1e-2,1e-3,1e-4\n      model.sequence_encoder.num_layers: 2,4,6\n      model.sequence_encoder.num_heads: 4,8\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-reproducible-training","title":"Problem: Reproducible Training","text":"<p>Goal: Get consistent results across training runs</p>"},{"location":"how-to-guides/model-training/#solution-1-set-seeds-properly","title":"Solution 1: Set Seeds Properly","text":"<pre><code># Use fixed seed\npython train.py +experiment=simple_mlp seed=42\n\n# Different experiments with different seeds\npython train.py +experiment=simple_mlp seed=42,123,456\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-enable-deterministic-training","title":"Solution 2: Enable Deterministic Training","text":"<pre><code># configs/trainer/deterministic.yaml\ndeterministic: true\nbenchmark: false\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-version-control-configurations","title":"Solution 3: Version Control Configurations","text":"<pre><code># Save exact configuration with each run\npython train.py +experiment=simple_mlp print_config=true\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-validating-model-performance","title":"Problem: Validating Model Performance","text":"<p>Goal: Properly evaluate your trained model</p>"},{"location":"how-to-guides/model-training/#solution-1-enable-testing-phase","title":"Solution 1: Enable Testing Phase","text":"<pre><code># Train and test in one command\npython train.py +experiment=simple_mlp train=true test=true\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-2-test-only-mode","title":"Solution 2: Test Only Mode","text":"<pre><code># Load and test a trained model\npython train.py +experiment=simple_mlp train=false test=true \\\n    ckpt_path=outputs/2024-01-01/12-00-00/.../checkpoints/last.ckpt\n</code></pre>"},{"location":"how-to-guides/model-training/#solution-3-cross-validation","title":"Solution 3: Cross-Validation","text":"<pre><code># Split data differently for validation\npython train.py +experiment=simple_mlp \\\n    data.train_val_test_split=[0.6,0.2,0.2]\n</code></pre>"},{"location":"how-to-guides/model-training/#problem-custom-training-configurations","title":"Problem: Custom Training Configurations","text":"<p>Goal: Create reusable custom training setups</p>"},{"location":"how-to-guides/model-training/#solution-create-your-own-experiment","title":"Solution: Create Your Own Experiment","text":"<pre><code># configs/experiments/my_setup.yaml\n# @package _global_\ndefaults:\n  - override /model: my_model_config\n  - override /data: my_data_config\n\ntags: [\"custom\", \"production\"]\n\nmodel:\n  learning_rate: 1e-4\n  weight_decay: 0.01\n\ntrainer:\n  max_epochs: 100\n  gradient_clip_val: 0.5\n  precision: 16-mixed\n\ndata:\n  batch_size: 64\n  num_workers: 4\n</code></pre> <p>Then use it: <pre><code>python train.py +experiment=my_setup\n</code></pre></p>"},{"location":"how-to-guides/model-training/#quick-reference-common-training-commands","title":"Quick Reference: Common Training Commands","text":"<pre><code># Fast development run\npython train.py +experiment=simple_mlp trainer.max_epochs=1 data.batch_size=8\n\n# Production training\npython train.py +experiment=transformer_small logger=wandb\n\n# Memory-efficient training\npython train.py +experiment=simple_mlp trainer.precision=16-mixed data.batch_size=32\n\n# Debug run\npython train.py +experiment=simple_mlp debug=true trainer.limit_train_batches=10\n\n# Hyperparameter sweep\npython train.py -m +experiment=simple_mlp model.learning_rate=1e-2,1e-3,1e-4\n</code></pre>"},{"location":"how-to-guides/model-training/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation: How to Evaluate Models</li> <li>Data Issues: How to Prepare Data</li> <li>Advanced: How to Configure Experiments </li> </ul>"},{"location":"how-to-guides/ssl-training/","title":"How-to: Self-Supervised Learning Training","text":"<p>This guide covers advanced techniques for training self-supervised learning models with Tabular SSL's corruption strategies.</p>"},{"location":"how-to-guides/ssl-training/#quick-start","title":"Quick Start","text":""},{"location":"how-to-guides/ssl-training/#run-demo-scripts-first","title":"Run Demo Scripts First","text":"<p>Before training, explore the corruption strategies interactively:</p> <pre><code># See how corruption strategies work\npython demo_corruption_strategies.py\n\n# Try with real credit card data\npython demo_credit_card_data.py\n</code></pre>"},{"location":"how-to-guides/ssl-training/#basic-ssl-training","title":"Basic SSL Training","text":"<pre><code># VIME: Mask estimation + value imputation\npython train.py +experiment=vime_ssl\n\n# SCARF: Contrastive learning\npython train.py +experiment=scarf_ssl\n\n# ReConTab: Multi-task reconstruction\npython train.py +experiment=recontab_ssl\n</code></pre>"},{"location":"how-to-guides/ssl-training/#choosing-the-right-strategy","title":"Choosing the Right Strategy","text":""},{"location":"how-to-guides/ssl-training/#vime-when-to-use","title":"VIME - When to Use","text":"<p>Best for: - Mixed categorical/numerical tabular data - Interpretable pretext tasks - Downstream tasks requiring feature reconstruction</p> <p>Characteristics: - Moderate corruption rate (30%) - Returns explicit masks - Two complementary tasks: mask estimation + value imputation</p> <pre><code>python train.py +experiment=vime_ssl\n</code></pre>"},{"location":"how-to-guides/ssl-training/#scarf-when-to-use","title":"SCARF - When to Use","text":"<p>Best for: - Large datasets with diverse features - Pure representation learning - High-dimensional tabular data</p> <p>Characteristics: - High corruption rate (60%+) - Contrastive learning approach - Requires larger batch sizes</p> <pre><code>python train.py +experiment=scarf_ssl\n</code></pre>"},{"location":"how-to-guides/ssl-training/#recontab-when-to-use","title":"ReConTab - When to Use","text":"<p>Best for: - Complex multi-task scenarios - Fine-grained corruption control - Hybrid reconstruction + contrastive approaches</p> <p>Characteristics: - Low base corruption rate (15%) but multiple types - Detailed corruption tracking - Flexible masking strategies</p> <pre><code>python train.py +experiment=recontab_ssl\n</code></pre>"},{"location":"how-to-guides/ssl-training/#customizing-corruption-parameters","title":"Customizing Corruption Parameters","text":""},{"location":"how-to-guides/ssl-training/#vime-customization","title":"VIME Customization","text":"<pre><code># Adjust corruption rate\npython train.py +experiment=vime_ssl model/corruption.corruption_rate=0.5\n\n# Use with different sequence lengths\npython train.py +experiment=vime_ssl data.sequence_length=64\n\n# Modify loss weights\npython train.py +experiment=vime_ssl model.mask_estimation_weight=2.0 model.value_imputation_weight=1.0\n</code></pre>"},{"location":"how-to-guides/ssl-training/#scarf-customization","title":"SCARF Customization","text":"<pre><code># Change corruption strategy\npython train.py +experiment=scarf_ssl model/corruption.corruption_strategy=marginal_sampling\n\n# Adjust temperature for contrastive loss\npython train.py +experiment=scarf_ssl model.temperature=0.05\n\n# Use larger batch size (important for SCARF)\npython train.py +experiment=scarf_ssl data.batch_size=256\n</code></pre>"},{"location":"how-to-guides/ssl-training/#recontab-customization","title":"ReConTab Customization","text":"<pre><code># Enable only specific corruption types\npython train.py +experiment=recontab_ssl model/corruption.corruption_types=['masking','noise']\n\n# Use column-wise masking\npython train.py +experiment=recontab_ssl model/corruption.masking_strategy=column_wise\n\n# Adjust individual corruption parameters\npython train.py +experiment=recontab_ssl model/corruption.noise_std=0.2 model/corruption.swap_probability=0.2\n</code></pre>"},{"location":"how-to-guides/ssl-training/#working-with-your-own-data","title":"Working with Your Own Data","text":""},{"location":"how-to-guides/ssl-training/#preparing-data-for-ssl","title":"Preparing Data for SSL","text":"<ol> <li>Create your DataModule:</li> </ol> <pre><code># configs/data/your_data.yaml\n_target_: tabular_ssl.data.base.DataModule\ndata_path: \"path/to/your/data.csv\"\nsequence_length: 32\nbatch_size: 64\n\n# Feature specifications\ncategorical_columns: [\"category_col1\", \"category_col2\"]\nnumerical_columns: [\"num_col1\", \"num_col2\", \"num_col3\"]\n\n# Sample data generation (optional)\nsample_data_config:\n  n_users: 1000\n  sequence_length: 32\n</code></pre> <ol> <li>Use with SSL experiments:</li> </ol> <pre><code># Use your data with VIME\npython train.py +experiment=vime_ssl data=your_data\n\n# Use your data with SCARF\npython train.py +experiment=scarf_ssl data=your_data\n</code></pre>"},{"location":"how-to-guides/ssl-training/#feature-type-detection","title":"Feature Type Detection","text":"<p>Corruption strategies need to know which features are categorical vs numerical:</p> <pre><code># Automatic detection (default)\npython train.py +experiment=vime_ssl\n\n# Manual specification\npython train.py +experiment=vime_ssl \\\n  model/corruption.categorical_indices=[0,1,2] \\\n  model/corruption.numerical_indices=[3,4,5,6,7]\n</code></pre>"},{"location":"how-to-guides/ssl-training/#advanced-training-techniques","title":"Advanced Training Techniques","text":""},{"location":"how-to-guides/ssl-training/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code># Use multiple GPUs\npython train.py +experiment=vime_ssl trainer.devices=2 trainer.strategy=ddp\n\n# Adjust batch size for multi-GPU\npython train.py +experiment=scarf_ssl trainer.devices=4 data.batch_size=512\n</code></pre>"},{"location":"how-to-guides/ssl-training/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>All SSL experiments support mixed precision for faster training:</p> <pre><code># Already enabled in experiments (precision: 16-mixed)\npython train.py +experiment=vime_ssl\n\n# Disable if needed\npython train.py +experiment=vime_ssl trainer.precision=32\n</code></pre>"},{"location":"how-to-guides/ssl-training/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Use Hydra's multirun for hyperparameter sweeps:</p> <pre><code># Sweep corruption rates for VIME\npython train.py +experiment=vime_ssl -m model/corruption.corruption_rate=0.1,0.3,0.5\n\n# Sweep SCARF parameters\npython train.py +experiment=scarf_ssl -m \\\n  model/corruption.corruption_rate=0.4,0.6,0.8 \\\n  model.temperature=0.05,0.1,0.2\n</code></pre>"},{"location":"how-to-guides/ssl-training/#monitoring-training","title":"Monitoring Training","text":""},{"location":"how-to-guides/ssl-training/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<p>VIME: - <code>train/mask_estimation_loss</code> - Should decrease steadily - <code>train/value_imputation_loss</code> - Should decrease steadily - <code>val/total_loss</code> - Overall validation performance</p> <p>SCARF: - <code>train/contrastive_loss</code> - Should decrease and stabilize - Representation quality metrics (if using downstream tasks)</p> <p>ReConTab: - <code>train/masked_reconstruction</code> - Masking reconstruction quality - <code>train/denoising</code> - Noise removal quality - <code>train/unswapping</code> - Feature unswapping quality</p>"},{"location":"how-to-guides/ssl-training/#using-weights-biases","title":"Using Weights &amp; Biases","text":"<p>SSL experiments are pre-configured for W&amp;B logging:</p> <pre><code># Logs automatically to your W&amp;B account\npython train.py +experiment=vime_ssl\n\n# Customize project name\npython train.py +experiment=vime_ssl logger.wandb.project=my-ssl-project\n</code></pre>"},{"location":"how-to-guides/ssl-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to-guides/ssl-training/#poor-convergence","title":"Poor Convergence","text":"<p>Problem: Training loss not decreasing</p> <p>Solutions: <pre><code># Lower learning rate\npython train.py +experiment=vime_ssl model.learning_rate=5e-5\n\n# Increase warmup steps\npython train.py +experiment=vime_ssl model.scheduler_type=cosine_with_warmup\n\n# Reduce corruption rate\npython train.py +experiment=vime_ssl model/corruption.corruption_rate=0.2\n</code></pre></p>"},{"location":"how-to-guides/ssl-training/#memory-issues","title":"Memory Issues","text":"<p>Problem: CUDA out of memory</p> <p>Solutions: <pre><code># Reduce batch size\npython train.py +experiment=scarf_ssl data.batch_size=32\n\n# Reduce sequence length\npython train.py +experiment=vime_ssl data.sequence_length=16\n\n# Use gradient accumulation\npython train.py +experiment=vime_ssl trainer.accumulate_grad_batches=2\n</code></pre></p>"},{"location":"how-to-guides/ssl-training/#scarf-specific-issues","title":"SCARF-Specific Issues","text":"<p>Problem: Contrastive loss not decreasing</p> <p>Solutions: <pre><code># Increase batch size (critical for SCARF)\npython train.py +experiment=scarf_ssl data.batch_size=256\n\n# Adjust temperature\npython train.py +experiment=scarf_ssl model.temperature=0.07\n\n# Increase corruption rate\npython train.py +experiment=scarf_ssl model/corruption.corruption_rate=0.8\n</code></pre></p>"},{"location":"how-to-guides/ssl-training/#evaluation-and-downstream-tasks","title":"Evaluation and Downstream Tasks","text":""},{"location":"how-to-guides/ssl-training/#save-trained-models","title":"Save Trained Models","text":"<p>SSL experiments automatically save checkpoints:</p> <pre><code># Training saves to outputs/YYYY-MM-DD/HH-MM-SS/\nls outputs/  # Find your experiment\n\n# Best checkpoint is saved automatically\nls outputs/2024-01-15/14-30-45/checkpoints/\n</code></pre>"},{"location":"how-to-guides/ssl-training/#extract-representations","title":"Extract Representations","text":"<pre><code>import torch\nfrom tabular_ssl.models.base import BaseModel\n\n# Load trained model\nmodel = BaseModel.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\nmodel.eval()\n\n# Extract representations\nwith torch.no_grad():\n    representations = model(your_data)\n</code></pre>"},{"location":"how-to-guides/ssl-training/#downstream-task-training","title":"Downstream Task Training","text":"<p>Use pre-trained SSL models for downstream tasks:</p> <pre><code># Load SSL checkpoint for fine-tuning\npython train.py +experiment=classification_finetune \\\n  model.ssl_checkpoint_path=outputs/2024-01-15/14-30-45/checkpoints/best.ckpt\n</code></pre>"},{"location":"how-to-guides/ssl-training/#custom-corruption-strategies","title":"Custom Corruption Strategies","text":""},{"location":"how-to-guides/ssl-training/#create-your-own-strategy","title":"Create Your Own Strategy","text":"<pre><code># custom_corruption.py\nimport torch\nimport torch.nn as nn\n\nclass CustomCorruption(nn.Module):\n    def __init__(self, corruption_rate: float = 0.2):\n        super().__init__()\n        self.corruption_rate = corruption_rate\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        if not self.training:\n            return x\n\n        # Your custom corruption logic\n        mask = torch.rand_like(x) &gt; self.corruption_rate\n        return x * mask\n</code></pre>"},{"location":"how-to-guides/ssl-training/#use-custom-strategy","title":"Use Custom Strategy","text":"<pre><code># configs/model/corruption/custom.yaml\n_target_: path.to.custom_corruption.CustomCorruption\ncorruption_rate: 0.2\n</code></pre> <pre><code>python train.py model/corruption=custom\n</code></pre>"},{"location":"how-to-guides/ssl-training/#best-practices","title":"Best Practices","text":""},{"location":"how-to-guides/ssl-training/#1-start-with-demos","title":"1. Start with Demos","text":"<p>Always run <code>demo_corruption_strategies.py</code> first to understand how each strategy works.</p>"},{"location":"how-to-guides/ssl-training/#2-use-appropriate-batch-sizes","title":"2. Use Appropriate Batch Sizes","text":"<ul> <li>VIME/ReConTab: 32-128 typically sufficient</li> <li>SCARF: 128+ recommended for effective contrastive learning</li> </ul>"},{"location":"how-to-guides/ssl-training/#3-monitor-feature-types","title":"3. Monitor Feature Types","text":"<p>Ensure categorical/numerical indices are correctly specified for optimal corruption.</p>"},{"location":"how-to-guides/ssl-training/#4-experiment-with-corruption-rates","title":"4. Experiment with Corruption Rates","text":"<ul> <li>Start with paper defaults</li> <li>Tune based on downstream task performance</li> <li>Higher rates aren't always better</li> </ul>"},{"location":"how-to-guides/ssl-training/#5-use-mixed-precision","title":"5. Use Mixed Precision","text":"<p>Enable <code>precision: 16-mixed</code> for 2x speedup with minimal quality loss.</p>"},{"location":"how-to-guides/ssl-training/#6-save-everything","title":"6. Save Everything","text":"<p>SSL training can be expensive - ensure checkpointing is enabled.</p>"},{"location":"how-to-guides/ssl-training/#paper-references","title":"Paper References","text":"<p>For implementation details and theoretical background:</p> <ul> <li>VIME: NeurIPS 2020</li> <li>SCARF: arXiv 2021</li> <li>General SSL: Self-Supervised Learning Survey </li> </ul>"},{"location":"reference/","title":"Reference Documentation","text":"<p>This section provides detailed technical documentation for all components of the Tabular SSL library.</p>"},{"location":"reference/#available-references","title":"Available References","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Models - Model architectures and configurations</li> <li>Data Utilities - Data loading and preprocessing utilities</li> <li>Utility Functions - Helper functions and tools</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#main-classes","title":"Main Classes","text":"<pre><code>from tabular_ssl import TabularSSL\nfrom tabular_ssl.data import DataLoader\n</code></pre>"},{"location":"reference/#common-functions","title":"Common Functions","text":"<pre><code>from tabular_ssl.utils import evaluate_model, plot_training_history\n</code></pre>"},{"location":"reference/#module-structure","title":"Module Structure","text":"<ul> <li><code>tabular_ssl/</code> - Main package</li> <li><code>models/</code> - Model implementations</li> <li><code>data/</code> - Data handling utilities</li> <li><code>utils/</code> - Helper functions</li> </ul>"},{"location":"reference/#related-resources","title":"Related Resources","text":"<ul> <li>Tutorials - Step-by-step guides</li> <li>How-to Guides - Practical solutions</li> <li>Explanation - Background information </li> </ul>"},{"location":"reference/api/","title":"API Reference","text":"<p>This section provides detailed technical documentation for all public APIs in the Tabular SSL library.</p>"},{"location":"reference/api/#models","title":"Models","text":""},{"location":"reference/api/#tabularssl","title":"TabularSSL","text":"<p>The main class for self-supervised learning on tabular data.</p> <pre><code>from tabular_ssl import TabularSSL\n</code></pre>"},{"location":"reference/api/#parameters","title":"Parameters","text":"<ul> <li><code>input_dim</code> (int): Dimension of input features</li> <li><code>hidden_dim</code> (int, optional): Dimension of hidden layers. Defaults to 256.</li> <li><code>num_layers</code> (int, optional): Number of transformer layers. Defaults to 4.</li> <li><code>num_heads</code> (int, optional): Number of attention heads. Defaults to 4.</li> <li><code>dropout</code> (float, optional): Dropout rate. Defaults to 0.1.</li> <li><code>mask_ratio</code> (float, optional): Ratio of features to mask during training. Defaults to 0.15.</li> </ul>"},{"location":"reference/api/#methods","title":"Methods","text":""},{"location":"reference/api/#traindata-batch_size32-epochs100-learning_rate1e-4","title":"<code>train(data, batch_size=32, epochs=100, learning_rate=1e-4)</code>","text":"<p>Train the model using self-supervised learning.</p> <p>Parameters: - <code>data</code> (pd.DataFrame): Input data - <code>batch_size</code> (int): Batch size for training - <code>epochs</code> (int): Number of training epochs - <code>learning_rate</code> (float): Learning rate</p> <p>Returns: - <code>dict</code>: Training history</p>"},{"location":"reference/api/#predictdata","title":"<code>predict(data)</code>","text":"<p>Make predictions on new data.</p> <p>Parameters: - <code>data</code> (pd.DataFrame): Input data</p> <p>Returns: - <code>np.ndarray</code>: Model predictions</p>"},{"location":"reference/api/#data-utilities","title":"Data Utilities","text":""},{"location":"reference/api/#dataloader","title":"DataLoader","text":"<p>Utility class for loading and preprocessing tabular data.</p> <pre><code>from tabular_ssl.data import DataLoader\n</code></pre>"},{"location":"reference/api/#methods_1","title":"Methods","text":""},{"location":"reference/api/#load_datafile_path-target_colnone","title":"<code>load_data(file_path, target_col=None)</code>","text":"<p>Load data from a file.</p> <p>Parameters: - <code>file_path</code> (str): Path to the data file - <code>target_col</code> (str, optional): Name of the target column</p> <p>Returns: - <code>pd.DataFrame</code>: Loaded data</p>"},{"location":"reference/api/#preprocessdata-categorical_colsnone","title":"<code>preprocess(data, categorical_cols=None)</code>","text":"<p>Preprocess the data.</p> <p>Parameters: - <code>data</code> (pd.DataFrame): Input data - <code>categorical_cols</code> (list, optional): List of categorical column names</p> <p>Returns: - <code>pd.DataFrame</code>: Preprocessed data</p>"},{"location":"reference/api/#utility-functions","title":"Utility Functions","text":""},{"location":"reference/api/#evaluation","title":"Evaluation","text":"<pre><code>from tabular_ssl.utils import evaluate_model\n</code></pre>"},{"location":"reference/api/#evaluate_modelmodel-test_data-metricsaccuracy-f1","title":"<code>evaluate_model(model, test_data, metrics=['accuracy', 'f1'])</code>","text":"<p>Evaluate model performance.</p> <p>Parameters: - <code>model</code>: Trained model - <code>test_data</code> (pd.DataFrame): Test data - <code>metrics</code> (list): List of metrics to compute</p> <p>Returns: - <code>dict</code>: Dictionary of metric scores</p>"},{"location":"reference/api/#visualization","title":"Visualization","text":"<pre><code>from tabular_ssl.utils import plot_training_history\n</code></pre>"},{"location":"reference/api/#plot_training_historyhistory","title":"<code>plot_training_history(history)</code>","text":"<p>Plot training history.</p> <p>Parameters: - <code>history</code> (dict): Training history dictionary</p> <p>Returns: - <code>matplotlib.figure.Figure</code>: Plot figure </p>"},{"location":"reference/config/","title":"Configuration Reference","text":"<p>This reference documents the configuration system and available configuration options in Tabular SSL.</p>"},{"location":"reference/config/#configuration-system","title":"Configuration System","text":"<p>Tabular SSL uses Hydra for configuration management, with Pydantic for validation.</p>"},{"location":"reference/config/#main-configuration-file","title":"Main Configuration File","text":"<p>The main configuration file (<code>configs/config.yaml</code>) is the entry point for configuration:</p> <pre><code># @package _global_\n\n# Default configurations that will be merged\ndefaults:\n  - _self_\n  - paths: default.yaml\n  - hydra: default.yaml\n  - model: default.yaml\n  - data: default.yaml\n  - trainer: default.yaml\n  - callbacks: default.yaml\n  - logger: default.yaml\n  - experiment: null  # No default experiment\n  - debug: null  # No debug mode by default\n\n# Project information\nproject_name: \"tabular-ssl\"\nproject_version: \"0.1.0\"\n\n# Training parameters\ntask_name: \"ssl\"\ntags: [\"tabular\", \"ssl\"]\nseed: 42\ndebug: false\n\n# Experiment tracking\nlog_dir: ${paths.log_dir}\ncheckpoint_dir: ${paths.checkpoint_dir}\n</code></pre>"},{"location":"reference/config/#component-configurations","title":"Component Configurations","text":""},{"location":"reference/config/#model-configuration","title":"Model Configuration","text":"<p>The model configuration (<code>configs/model/default.yaml</code>) specifies the model architecture:</p> <pre><code># configs/model/default.yaml\ndefaults:\n  - _self_\n  - event_encoder: mlp.yaml\n  - sequence_encoder: transformer.yaml\n  - embedding: categorical.yaml\n  - projection_head: mlp.yaml\n  - prediction_head: classification.yaml\n\n_target_: tabular_ssl.models.base.BaseModel\n\nmodel:\n  name: tabular_ssl_model\n  type: base\n  event_encoder: ${event_encoder}\n  sequence_encoder: ${sequence_encoder}\n  embedding: ${embedding}\n  projection_head: ${projection_head}\n  prediction_head: ${prediction_head}\n\n  # Optimizer settings\n  optimizer:\n    _target_: torch.optim.Adam\n    lr: 1.0e-3\n    weight_decay: 0.0\n\n  # Learning rate scheduler\n  lr_scheduler:\n    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n    mode: min\n    factor: 0.5\n    patience: 5\n</code></pre>"},{"location":"reference/config/#event-encoder-configurations","title":"Event Encoder Configurations","text":""},{"location":"reference/config/#mlp-event-encoder","title":"MLP Event Encoder","text":"<pre><code># configs/model/event_encoder/mlp.yaml\nname: mlp_encoder\ntype: mlp_event_encoder\ninput_dim: 64\nhidden_dims: [128, 256]\noutput_dim: 512\ndropout: 0.1\nuse_batch_norm: true\n</code></pre>"},{"location":"reference/config/#autoencoder-event-encoder","title":"Autoencoder Event Encoder","text":"<pre><code># configs/model/event_encoder/autoencoder.yaml\nname: autoencoder_encoder\ntype: autoencoder\ninput_dim: 64\nhidden_dims: [128, 64]\noutput_dim: 32\ndropout: 0.1\nuse_batch_norm: true\nuse_reconstruction_loss: true\n</code></pre>"},{"location":"reference/config/#contrastive-event-encoder","title":"Contrastive Event Encoder","text":"<pre><code># configs/model/event_encoder/contrastive.yaml\nname: contrastive_encoder\ntype: contrastive\ninput_dim: 64\nhidden_dims: [128, 64]\noutput_dim: 32\ndropout: 0.1\nuse_batch_norm: true\ntemperature: 0.07\n</code></pre>"},{"location":"reference/config/#sequence-encoder-configurations","title":"Sequence Encoder Configurations","text":""},{"location":"reference/config/#transformer-encoder","title":"Transformer Encoder","text":"<pre><code># configs/model/sequence_encoder/transformer.yaml\nname: transformer_encoder\ntype: transformer\ninput_dim: 512\nhidden_dim: 512\nnum_layers: 4\nnum_heads: 8\ndim_feedforward: 2048\ndropout: 0.1\nbidirectional: true\n</code></pre>"},{"location":"reference/config/#lstm-encoder","title":"LSTM Encoder","text":"<pre><code># configs/model/sequence_encoder/lstm.yaml\nname: lstm_encoder\ntype: lstm\ninput_dim: 512\nhidden_dim: 512\nnum_layers: 2\ndropout: 0.1\nbidirectional: true\n</code></pre>"},{"location":"reference/config/#s4-encoder","title":"S4 Encoder","text":"<pre><code># configs/model/sequence_encoder/s4.yaml\nname: s4_encoder\ntype: s4\nd_model: 512\nd_state: 64\ndropout: 0.1\nbidirectional: true\nmax_sequence_length: 2048\nuse_checkpoint: false\n</code></pre>"},{"location":"reference/config/#embedding-configurations","title":"Embedding Configurations","text":"<pre><code># configs/model/embedding/categorical.yaml\nname: categorical_embedding\ntype: categorical_embedding\nembedding_dims:\n  - [5, 8]  # 5 categories, 8-dimensional embedding\n  - [3, 4]  # 3 categories, 4-dimensional embedding\ndropout: 0.1\n</code></pre>"},{"location":"reference/config/#projection-head-configurations","title":"Projection Head Configurations","text":"<pre><code># configs/model/projection_head/mlp.yaml\nname: mlp_projection\ntype: mlp_projection\ninput_dim: 512\nhidden_dims: [256]\noutput_dim: 128\ndropout: 0.1\nuse_batch_norm: true\n</code></pre>"},{"location":"reference/config/#prediction-head-configurations","title":"Prediction Head Configurations","text":"<pre><code># configs/model/prediction_head/classification.yaml\nname: classification_head\ntype: classification\ninput_dim: 128\nnum_classes: 2\nhidden_dims: [64]\ndropout: 0.1\nuse_batch_norm: true\n</code></pre>"},{"location":"reference/config/#data-configurations","title":"Data Configurations","text":"<pre><code># configs/data/default.yaml\n_target_: tabular_ssl.data.TabularDataModule\n\ndata:\n  name: default_dataset\n  path: ${paths.data_dir}/dataset.csv\n  train_ratio: 0.8\n  val_ratio: 0.1\n  test_ratio: 0.1\n  batch_size: 32\n  num_workers: 4\n  shuffle: true\n  drop_last: false\n  normalize: true\n  categorical_columns: []\n  numerical_columns: []\n  target_column: null  # For supervised learning\n</code></pre>"},{"location":"reference/config/#trainer-configurations","title":"Trainer Configurations","text":"<pre><code># configs/trainer/default.yaml\n_target_: pytorch_lightning.Trainer\n\ntrainer:\n  accelerator: auto\n  strategy: auto\n  devices: auto\n  num_nodes: 1\n  precision: 32\n  max_epochs: 100\n  min_epochs: 1\n  max_steps: -1\n  min_steps: null\n  limit_train_batches: 1.0\n  limit_val_batches: 1.0\n  limit_test_batches: 1.0\n  limit_predict_batches: 1.0\n  fast_dev_run: false\n  overfit_batches: 0.0\n  val_check_interval: 1.0\n  check_val_every_n_epoch: 1\n  num_sanity_val_steps: 2\n  log_every_n_steps: 50\n  enable_checkpointing: true\n  enable_progress_bar: true\n  enable_model_summary: true\n  accumulate_grad_batches: 1\n  gradient_clip_val: null\n  gradient_clip_algorithm: norm\n  deterministic: false\n  benchmark: false\n  inference_mode: true\n  use_distributed_sampler: true\n  detect_anomaly: false\n  barebones: false\n  plugins: null\n  sync_batchnorm: false\n  reload_dataloaders_every_n_epochs: 0\n</code></pre>"},{"location":"reference/config/#callbacks-configurations","title":"Callbacks Configurations","text":"<pre><code># configs/callbacks/default.yaml\ndefaults:\n  - _self_\n\ncallbacks:\n  model_checkpoint:\n    _target_: pytorch_lightning.callbacks.ModelCheckpoint\n    dirpath: ${checkpoint_dir}\n    filename: \"epoch_{epoch:03d}-val_loss_{val/loss:.4f}\"\n    monitor: \"val/loss\"\n    mode: \"min\"\n    save_last: true\n    save_top_k: 3\n    auto_insert_metric_name: false\n\n  early_stopping:\n    _target_: pytorch_lightning.callbacks.EarlyStopping\n    monitor: \"val/loss\"\n    patience: 10\n    mode: \"min\"\n    min_delta: 0.0001\n\n  lr_monitor:\n    _target_: pytorch_lightning.callbacks.LearningRateMonitor\n    logging_interval: \"epoch\"\n</code></pre>"},{"location":"reference/config/#logger-configurations","title":"Logger Configurations","text":"<pre><code># configs/logger/default.yaml\ndefaults:\n  - _self_\n\nlogger:\n  tensorboard:\n    _target_: pytorch_lightning.loggers.TensorBoardLogger\n    save_dir: ${log_dir}\n    name: null\n    version: null\n    log_graph: false\n    default_hp_metric: true\n    prefix: \"\"\n</code></pre>"},{"location":"reference/config/#experiment-configurations","title":"Experiment Configurations","text":"<p>Experiment configurations are stored in <code>configs/experiment/</code> and can override any of the above configurations.</p> <pre><code># configs/experiment/transformer_ssl.yaml\n# @package _global_\n\ndefaults:\n  - override /model/event_encoder: mlp.yaml\n  - override /model/sequence_encoder: transformer.yaml\n  - override /trainer: default.yaml\n  - override /model: default.yaml\n  - override /callbacks: default.yaml\n  - _self_\n\ntags: [\"transformer\", \"ssl\"]\n\nseed: 12345\n\ntrainer:\n  max_epochs: 100\n  gradient_clip_val: 0.5\n\nmodel:\n  optimizer:\n    lr: 1.0e-4\n    weight_decay: 0.01\n</code></pre>"},{"location":"reference/config/#debug-configurations","title":"Debug Configurations","text":"<p>Debug configurations provide settings for development and debugging:</p> <pre><code># configs/debug/default.yaml\n# @package _global_\n\n# Enable debug mode\ndebug: true\n\n# Reduce dataset size for faster iterations\ndata:\n  train_ratio: 0.05\n  val_ratio: 0.05\n  test_ratio: 0.05\n  batch_size: 8\n  num_workers: 0\n\n# Reduce training time\ntrainer:\n  max_epochs: 5\n  limit_train_batches: 10\n  limit_val_batches: 10\n  limit_test_batches: 10\n  log_every_n_steps: 1\n  num_sanity_val_steps: 0\n\n# Disable checkpointing\ncallbacks:\n  model_checkpoint:\n    save_top_k: 1\n    every_n_epochs: 1\n</code></pre>"},{"location":"reference/config/#environment-variables","title":"Environment Variables","text":"<p>The configuration system supports environment variables using the <code>${oc.env:VAR_NAME,default_value}</code> syntax.</p> <p>For example:</p> <pre><code>paths:\n  data_dir: ${oc.env:DATA_DIR,${project_path}/data}\n</code></pre>"},{"location":"reference/config/#command-line-overrides","title":"Command-line Overrides","text":"<p>Any configuration parameter can be overridden from the command line:</p> <pre><code>python src/train.py model.optimizer.lr=0.001 trainer.max_epochs=50\n</code></pre>"},{"location":"reference/config/#multi-run-parameter-sweeps","title":"Multi-run (Parameter Sweeps)","text":"<p>Multiple runs with different parameters can be executed using the <code>-m</code> flag:</p> <pre><code>python src/train.py -m model.optimizer.lr=1e-3,1e-4,1e-5\n</code></pre>"},{"location":"reference/config/#see-also","title":"See Also","text":"<ul> <li>Configuring Experiments: Guide for creating and running experiments</li> <li>Hydra Documentation: Official Hydra documentation </li> </ul>"},{"location":"reference/corruption-strategies/","title":"Corruption Strategies Reference","text":"<p>This reference documents the corruption strategies implemented in Tabular SSL for self-supervised learning on tabular data.</p>"},{"location":"reference/corruption-strategies/#overview","title":"Overview","text":"<p>Corruption strategies are the foundation of self-supervised learning for tabular data. They create pretext tasks by transforming input data in specific ways, allowing models to learn meaningful representations without labeled data.</p>"},{"location":"reference/corruption-strategies/#available-strategies","title":"Available Strategies","text":""},{"location":"reference/corruption-strategies/#vime-corruption","title":"VIME Corruption","text":"<p>Paper: \"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain\" (NeurIPS 2020)</p>"},{"location":"reference/corruption-strategies/#purpose","title":"Purpose","text":"<p>VIME creates two complementary pretext tasks: 1. Mask Estimation: Predict which features were corrupted 2. Value Imputation: Reconstruct original feature values</p>"},{"location":"reference/corruption-strategies/#implementation","title":"Implementation","text":"<pre><code>from tabular_ssl.models.components import VIMECorruption\n\ncorruption = VIMECorruption(\n    corruption_rate=0.3,\n    categorical_indices=[0, 1, 2],\n    numerical_indices=[3, 4, 5, 6],\n    categorical_vocab_sizes={0: 100, 1: 50, 2: 20},\n    numerical_distributions={3: (0.5, 1.2), 4: (10.0, 5.0)}\n)\n\n# Apply corruption\ncorrupted_data, mask = corruption(data)\n</code></pre>"},{"location":"reference/corruption-strategies/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>corruption_rate</code> float 0.15 Fraction of features to corrupt <code>categorical_indices</code> List[int] None Indices of categorical features <code>numerical_indices</code> List[int] None Indices of numerical features <code>categorical_vocab_sizes</code> Dict[int, int] None Vocabulary sizes for categorical features <code>numerical_distributions</code> Dict[int, Tuple[float, float]] None (mean, std) for numerical features"},{"location":"reference/corruption-strategies/#outputs","title":"Outputs","text":"<ul> <li>corrupted_data: Input data with some features corrupted</li> <li>mask: Binary tensor indicating corrupted positions (1=corrupted, 0=original)</li> </ul>"},{"location":"reference/corruption-strategies/#feature-corruption-logic","title":"Feature Corruption Logic","text":"<p>Categorical Features: Replace with random value from vocabulary <pre><code>random_categories = torch.randint(0, vocab_size, shape)\n</code></pre></p> <p>Numerical Features: Replace with random value from feature distribution <pre><code>random_values = torch.normal(mean, std, shape)\n</code></pre></p>"},{"location":"reference/corruption-strategies/#configuration","title":"Configuration","text":"<pre><code># configs/model/corruption/vime.yaml\n_target_: tabular_ssl.models.components.VIMECorruption\ncorruption_rate: 0.3\ncategorical_indices: null  # Auto-detected\nnumerical_indices: null    # Auto-detected\n</code></pre>"},{"location":"reference/corruption-strategies/#scarf-corruption","title":"SCARF Corruption","text":"<p>Paper: \"SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption\" (arXiv 2021)</p>"},{"location":"reference/corruption-strategies/#purpose_1","title":"Purpose","text":"<p>SCARF optimizes representations for contrastive learning by corrupting features through replacement with values from other samples in the batch.</p>"},{"location":"reference/corruption-strategies/#implementation_1","title":"Implementation","text":"<pre><code>from tabular_ssl.models.components import SCARFCorruption\n\ncorruption = SCARFCorruption(\n    corruption_rate=0.6,\n    corruption_strategy=\"random_swap\"  # or \"marginal_sampling\"\n)\n\n# Single corruption\ncorrupted_data = corruption(data)\n\n# Contrastive pairs\nview1, view2 = corruption.create_contrastive_pairs(data)\n</code></pre>"},{"location":"reference/corruption-strategies/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>corruption_rate</code> float 0.6 Fraction of features to corrupt <code>corruption_strategy</code> str \"random_swap\" \"random_swap\" or \"marginal_sampling\""},{"location":"reference/corruption-strategies/#corruption-strategies","title":"Corruption Strategies","text":"<p>Random Swap: Randomly permute feature values across samples <pre><code>feature_values = x[:, :, feat_idx].flatten()\nperm_indices = torch.randperm(len(feature_values))\nshuffled_values = feature_values[perm_indices]\n</code></pre></p> <p>Marginal Sampling: Sample from marginal distribution of each feature <pre><code>feature_values = x[:, :, feat_idx].flatten()\nsample_indices = torch.randint(0, len(feature_values), shape)\nsampled_values = feature_values[sample_indices]\n</code></pre></p>"},{"location":"reference/corruption-strategies/#configuration_1","title":"Configuration","text":"<pre><code># configs/model/corruption/scarf.yaml\n_target_: tabular_ssl.models.components.SCARFCorruption\ncorruption_rate: 0.6\ncorruption_strategy: \"random_swap\"\ntemperature: 0.1  # For contrastive loss\n</code></pre>"},{"location":"reference/corruption-strategies/#recontab-corruption","title":"ReConTab Corruption","text":"<p>Purpose: Multi-task reconstruction-based learning combining multiple corruption types with detailed tracking for reconstruction targets.</p>"},{"location":"reference/corruption-strategies/#implementation_2","title":"Implementation","text":"<pre><code>from tabular_ssl.models.components import ReConTabCorruption\n\ncorruption = ReConTabCorruption(\n    corruption_rate=0.15,\n    categorical_indices=[0, 1, 2],\n    numerical_indices=[3, 4, 5, 6],\n    corruption_types=[\"masking\", \"noise\", \"swapping\"],\n    masking_strategy=\"random\",\n    noise_std=0.1,\n    swap_probability=0.1\n)\n\n# Apply corruption\ncorrupted_data, corruption_info = corruption(data)\n\n# Get reconstruction targets\ntargets = corruption.reconstruction_targets(original_data, corrupted_data, corruption_info)\n</code></pre>"},{"location":"reference/corruption-strategies/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>corruption_rate</code> float 0.15 Base corruption rate for masking <code>categorical_indices</code> List[int] None Indices of categorical features <code>numerical_indices</code> List[int] None Indices of numerical features <code>corruption_types</code> List[str] [\"masking\", \"noise\", \"swapping\"] Types of corruption to apply <code>masking_strategy</code> str \"random\" \"random\", \"column_wise\", or \"block\" <code>noise_std</code> float 0.1 Standard deviation for Gaussian noise <code>swap_probability</code> float 0.1 Probability of swapping each feature"},{"location":"reference/corruption-strategies/#corruption-types","title":"Corruption Types","text":"<p>Masking: Zero out selected features <pre><code>mask = torch.bernoulli(torch.full(shape, corruption_rate))\nx_corrupted = x * (1 - mask)\n</code></pre></p> <p>Noise Injection: Add Gaussian noise to numerical features <pre><code>noise = torch.randn_like(x) * noise_std\nx_corrupted = x + noise\n</code></pre></p> <p>Feature Swapping: Randomly permute features across samples <pre><code>perm_indices = torch.randperm(batch_size)\nx_corrupted[:, :, feat_idx] = x[perm_indices, :, feat_idx]\n</code></pre></p>"},{"location":"reference/corruption-strategies/#masking-strategies","title":"Masking Strategies","text":"<p>Random: Randomly mask individual elements Column-wise: Mask entire features (columns) Block: Mask contiguous temporal blocks</p>"},{"location":"reference/corruption-strategies/#outputs_1","title":"Outputs","text":"<ul> <li>corrupted_data: Input data with applied corruptions</li> <li>corruption_info: Tensor indicating corruption type for each element</li> <li>0: Original (no corruption)</li> <li>1: Masked</li> <li>2: Noise added</li> <li>3: Swapped</li> </ul>"},{"location":"reference/corruption-strategies/#reconstruction-targets","title":"Reconstruction Targets","text":"<pre><code>targets = {\n    \"masked_values\": original[mask_positions],\n    \"mask_positions\": mask_positions,\n    \"denoised_values\": original[noise_positions],\n    \"noise_positions\": noise_positions,\n    \"unswapped_values\": original[swap_positions],\n    \"swap_positions\": swap_positions\n}\n</code></pre>"},{"location":"reference/corruption-strategies/#configuration_2","title":"Configuration","text":"<pre><code># configs/model/corruption/recontab.yaml\n_target_: tabular_ssl.models.components.ReConTabCorruption\ncorruption_rate: 0.15\ncorruption_types: [\"masking\", \"noise\", \"swapping\"]\nmasking_strategy: \"random\"\nnoise_std: 0.1\nswap_probability: 0.1\n</code></pre>"},{"location":"reference/corruption-strategies/#simple-corruption-strategies","title":"Simple Corruption Strategies","text":""},{"location":"reference/corruption-strategies/#random-masking","title":"Random Masking","text":"<p>Basic random feature masking:</p> <pre><code>from tabular_ssl.models.components import RandomMasking\n\nmasking = RandomMasking(corruption_rate=0.15)\nmasked_data = masking(data)\n</code></pre>"},{"location":"reference/corruption-strategies/#gaussian-noise","title":"Gaussian Noise","text":"<p>Add Gaussian noise to numerical features:</p> <pre><code>from tabular_ssl.models.components import GaussianNoise\n\nnoise = GaussianNoise(noise_std=0.1)\nnoisy_data = noise(data)\n</code></pre>"},{"location":"reference/corruption-strategies/#swapping-corruption","title":"Swapping Corruption","text":"<p>Random feature swapping between samples:</p> <pre><code>from tabular_ssl.models.components import SwappingCorruption\n\nswapping = SwappingCorruption(swap_prob=0.15)\nswapped_data = swapping(data)\n</code></pre>"},{"location":"reference/corruption-strategies/#usage-in-training","title":"Usage in Training","text":""},{"location":"reference/corruption-strategies/#vime-training-loop","title":"VIME Training Loop","text":"<pre><code>def vime_training_step(batch):\n    # Apply VIME corruption\n    corrupted_data, mask = vime_corruption(batch)\n\n    # Forward pass\n    representations = model(corrupted_data)\n\n    # VIME-specific heads\n    mask_pred = mask_estimation_head(representations)\n    reconstructed = value_imputation_head(representations)\n\n    # Compute losses\n    mask_loss = F.binary_cross_entropy_with_logits(mask_pred, mask)\n    recon_loss = F.mse_loss(reconstructed, batch)\n\n    return mask_loss + recon_loss\n</code></pre>"},{"location":"reference/corruption-strategies/#scarf-training-loop","title":"SCARF Training Loop","text":"<pre><code>def scarf_training_step(batch):\n    # Create contrastive pairs\n    view1, view2 = scarf_corruption.create_contrastive_pairs(batch)\n\n    # Get representations\n    z1 = model(view1)\n    z2 = model(view2)\n\n    # Contrastive loss\n    loss = contrastive_loss(z1, z2, temperature=0.1)\n    return loss\n</code></pre>"},{"location":"reference/corruption-strategies/#recontab-training-loop","title":"ReConTab Training Loop","text":"<pre><code>def recontab_training_step(batch):\n    # Apply multi-corruption\n    corrupted_data, corruption_info = recontab_corruption(batch)\n\n    # Forward pass\n    representations = model(corrupted_data)\n\n    # Get reconstruction targets\n    targets = recontab_corruption.reconstruction_targets(\n        batch, corrupted_data, corruption_info\n    )\n\n    # Multi-task reconstruction\n    losses = {}\n    if \"masked_values\" in targets:\n        pred = masked_reconstruction_head(representations)\n        losses[\"mask\"] = F.mse_loss(pred[targets[\"mask_positions\"]], \n                                   targets[\"masked_values\"])\n\n    # ... similar for denoising and unswapping\n\n    return sum(losses.values())\n</code></pre>"},{"location":"reference/corruption-strategies/#choosing-corruption-strategies","title":"Choosing Corruption Strategies","text":""},{"location":"reference/corruption-strategies/#vime","title":"VIME","text":"<p>Best for:  - Mixed categorical/numerical tabular data - When you want explicit mask prediction capability - Interpretable pretext tasks</p> <p>Typical corruption rate: 0.3</p>"},{"location":"reference/corruption-strategies/#scarf","title":"SCARF","text":"<p>Best for: - Large datasets with diverse feature distributions - When contrastive learning is preferred - High-dimensional tabular data</p> <p>Typical corruption rate: 0.6+</p>"},{"location":"reference/corruption-strategies/#recontab","title":"ReConTab","text":"<p>Best for: - Complex multi-task scenarios - When you want fine-grained corruption control - Combining reconstruction with other objectives</p> <p>Typical corruption rate: 0.15 (base rate, actual varies by corruption type)</p>"},{"location":"reference/corruption-strategies/#performance-tips","title":"Performance Tips","text":"<ol> <li>Corruption Rate: Start with paper defaults, then tune based on validation performance</li> <li>Feature Types: Ensure correct categorical/numerical feature specification</li> <li>Batch Size: SCARF benefits from larger batch sizes (128+) for effective contrastive learning</li> <li>Mixed Precision: All strategies support <code>precision: 16-mixed</code> for faster training</li> <li>Distribution Estimation: For VIME, set feature distributions from training data for best results</li> </ol>"},{"location":"reference/corruption-strategies/#demo-scripts","title":"Demo Scripts","text":"<p>Run interactive demos to understand each strategy:</p> <pre><code># Compare all strategies interactively\npython demo_corruption_strategies.py\n\n# Real data demo\npython demo_credit_card_data.py\n</code></pre>"},{"location":"reference/corruption-strategies/#paper-references","title":"Paper References","text":"<ul> <li> <p>VIME: Yoon, J., Zhang, Y., Jordon, J., &amp; van der Schaar, M. (2020). VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain. NeurIPS 2020.</p> </li> <li> <p>SCARF: Bahri, D., Jiang, H., Tay, Y., &amp; Metzler, D. (2021). SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption for Representation Learning. arXiv:2106.15147. </p> </li> </ul>"},{"location":"reference/data/","title":"Data Utilities Reference","text":"<p>This section provides detailed documentation of the data loading and preprocessing utilities in Tabular SSL.</p>"},{"location":"reference/data/#dataloader","title":"DataLoader","text":"<p>The main class for loading and preprocessing tabular data.</p> <pre><code>from tabular_ssl.data import DataLoader\n</code></pre>"},{"location":"reference/data/#methods","title":"Methods","text":""},{"location":"reference/data/#load_datafile_path-target_colnone","title":"<code>load_data(file_path, target_col=None)</code>","text":"<p>Load data from a file.</p> <p>Parameters: - <code>file_path</code> (str): Path to the data file - <code>target_col</code> (str, optional): Name of the target column</p> <p>Returns: - <code>pd.DataFrame</code>: Loaded data</p>"},{"location":"reference/data/#preprocessdata-categorical_colsnone-scale_numericaltrue-handle_missingtrue","title":"<code>preprocess(data, categorical_cols=None, scale_numerical=True, handle_missing=True)</code>","text":"<p>Preprocess the data.</p> <p>Parameters: - <code>data</code> (pd.DataFrame): Input data - <code>categorical_cols</code> (list, optional): List of categorical column names - <code>scale_numerical</code> (bool, optional): Whether to scale numerical features - <code>handle_missing</code> (bool, optional): Whether to handle missing values</p> <p>Returns: - <code>pd.DataFrame</code>: Preprocessed data</p>"},{"location":"reference/data/#data-transformers","title":"Data Transformers","text":""},{"location":"reference/data/#categoricaltransformer","title":"CategoricalTransformer","text":"<pre><code>from tabular_ssl.data import CategoricalTransformer\n\ntransformer = CategoricalTransformer(\n    columns=['category1', 'category2'],\n    encoding='onehot'\n)\n</code></pre>"},{"location":"reference/data/#numericaltransformer","title":"NumericalTransformer","text":"<pre><code>from tabular_ssl.data import NumericalTransformer\n\ntransformer = NumericalTransformer(\n    columns=['numeric1', 'numeric2'],\n    scaling='standard'\n)\n</code></pre>"},{"location":"reference/data/#data-validation","title":"Data Validation","text":""},{"location":"reference/data/#datavalidator","title":"DataValidator","text":"<pre><code>from tabular_ssl.data import DataValidator\n\nvalidator = DataValidator(\n    required_columns=['col1', 'col2'],\n    data_types={\n        'col1': 'numeric',\n        'col2': 'categorical'\n    }\n)\n</code></pre>"},{"location":"reference/data/#data-splitting","title":"Data Splitting","text":""},{"location":"reference/data/#datasplitter","title":"DataSplitter","text":"<pre><code>from tabular_ssl.data import DataSplitter\n\nsplitter = DataSplitter(\n    test_size=0.2,\n    val_size=0.1,\n    random_state=42\n)\n</code></pre>"},{"location":"reference/data/#feature-engineering","title":"Feature Engineering","text":""},{"location":"reference/data/#featureengineer","title":"FeatureEngineer","text":"<pre><code>from tabular_ssl.data import FeatureEngineer\n\nengineer = FeatureEngineer(\n    interactions=True,\n    polynomials=True,\n    degree=2\n)\n</code></pre>"},{"location":"reference/data/#data-augmentation","title":"Data Augmentation","text":""},{"location":"reference/data/#dataaugmenter","title":"DataAugmenter","text":"<pre><code>from tabular_ssl.data import DataAugmenter\n\naugmenter = DataAugmenter(\n    noise_level=0.1,\n    mask_ratio=0.15\n)\n</code></pre>"},{"location":"reference/data/#common-operations","title":"Common Operations","text":""},{"location":"reference/data/#loading-data","title":"Loading Data","text":"<pre><code># Load from CSV\ndata = DataLoader().load_data('data.csv')\n\n# Load from DataFrame\ndata = DataLoader().load_data(df)\n</code></pre>"},{"location":"reference/data/#preprocessing","title":"Preprocessing","text":"<pre><code># Basic preprocessing\nprocessed_data = DataLoader().preprocess(\n    data,\n    categorical_cols=['category1', 'category2']\n)\n\n# Advanced preprocessing\nprocessed_data = DataLoader().preprocess(\n    data,\n    categorical_cols=['category1', 'category2'],\n    scale_numerical=True,\n    handle_missing=True,\n    missing_strategy='mean'\n)\n</code></pre>"},{"location":"reference/data/#data-splitting_1","title":"Data Splitting","text":"<pre><code># Split data\ntrain_data, val_data, test_data = DataSplitter().split(data)\n</code></pre>"},{"location":"reference/data/#feature-engineering_1","title":"Feature Engineering","text":"<pre><code># Create new features\nengineered_data = FeatureEngineer().transform(data)\n</code></pre>"},{"location":"reference/data/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate data before processing</li> <li>Handle missing values appropriately</li> <li>Scale numerical features</li> <li>Encode categorical variables</li> <li>Split data before preprocessing</li> <li>Document preprocessing steps</li> <li>Save preprocessed data</li> <li>Use appropriate data types</li> </ol>"},{"location":"reference/data/#related-resources","title":"Related Resources","text":"<ul> <li>API Reference - Complete API documentation</li> <li>How-to Guides - Data preparation guides</li> <li>Tutorials - Getting started guides </li> </ul>"},{"location":"reference/models/","title":"Models Reference","text":"<p>This section provides detailed documentation of the model components and architecture available in Tabular SSL.</p>"},{"location":"reference/models/#architecture-overview","title":"Architecture Overview","text":"<p>Tabular SSL uses a simplified modular architecture where components are directly instantiated using Hydra's <code>_target_</code> mechanism. This approach is cleaner and more straightforward than the previous registry-based system.</p>"},{"location":"reference/models/#base-components","title":"Base Components","text":""},{"location":"reference/models/#basemodel","title":"<code>BaseModel</code>","text":"<p>The main model class that orchestrates all components.</p> <pre><code>from tabular_ssl.models.base import BaseModel\n</code></pre>"},{"location":"reference/models/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    event_encoder,\n    sequence_encoder=None,\n    embedding=None,\n    projection_head=None,\n    prediction_head=None,\n    learning_rate: float = 1e-4,\n    weight_decay: float = 0.01,\n    optimizer_type: str = \"adamw\",\n    scheduler_type: str = \"cosine\"\n):\n</code></pre>"},{"location":"reference/models/#key-methods","title":"Key Methods","text":"<ul> <li><code>forward(x)</code>: Main forward pass through all components</li> <li><code>training_step(batch, batch_idx)</code>: PyTorch Lightning training step</li> <li><code>validation_step(batch, batch_idx)</code>: PyTorch Lightning validation step</li> <li><code>configure_optimizers()</code>: Optimizer and scheduler configuration</li> </ul>"},{"location":"reference/models/#eventencoder","title":"<code>EventEncoder</code>","text":"<p>Base class for components that encode individual events or timesteps.</p> <pre><code>from tabular_ssl.models.base import EventEncoder\n</code></pre> <p>All event encoders should inherit from this class and implement the <code>forward</code> method:</p> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        x: Input tensor of shape (batch_size, seq_len, input_dim)\n    Returns:\n        Encoded tensor of shape (batch_size, seq_len, output_dim)\n    \"\"\"\n</code></pre>"},{"location":"reference/models/#sequenceencoder","title":"<code>SequenceEncoder</code>","text":"<p>Base class for components that encode sequences of events.</p> <pre><code>from tabular_ssl.models.base import SequenceEncoder\n</code></pre> <p>Sequence encoders process the output of event encoders:</p> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        x: Input tensor of shape (batch_size, seq_len, input_dim)\n    Returns:\n        Encoded sequence of shape (batch_size, seq_len, output_dim)\n    \"\"\"\n</code></pre>"},{"location":"reference/models/#embeddinglayer","title":"<code>EmbeddingLayer</code>","text":"<p>Base class for embedding layers that handle categorical features.</p> <pre><code>from tabular_ssl.models.base import EmbeddingLayer\n</code></pre>"},{"location":"reference/models/#projectionhead","title":"<code>ProjectionHead</code>","text":"<p>Base class for projection heads that transform representations.</p> <pre><code>from tabular_ssl.models.base import ProjectionHead\n</code></pre>"},{"location":"reference/models/#predictionhead","title":"<code>PredictionHead</code>","text":"<p>Base class for prediction heads that generate final outputs.</p> <pre><code>from tabular_ssl.models.base import PredictionHead\n</code></pre>"},{"location":"reference/models/#available-components","title":"Available Components","text":""},{"location":"reference/models/#event-encoders","title":"Event Encoders","text":""},{"location":"reference/models/#mlpeventencoder","title":"<code>MLPEventEncoder</code>","text":"<p>Multi-layer perceptron for encoding individual events.</p> <pre><code>from tabular_ssl.models.components import MLPEventEncoder\n</code></pre> <p>Constructor Parameters: - <code>input_dim</code> (int): Input feature dimension - <code>hidden_dims</code> (List[int]): List of hidden layer dimensions - <code>output_dim</code> (int): Output dimension - <code>dropout</code> (float): Dropout rate (default: 0.1) - <code>activation</code> (str): Activation function (\"relu\", \"gelu\", \"leaky_relu\") - <code>use_batch_norm</code> (bool): Whether to use batch normalization</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.MLPEventEncoder\ninput_dim: 64\nhidden_dims: [128, 256]\noutput_dim: 512\ndropout: 0.1\nactivation: relu\nuse_batch_norm: true\n</code></pre></p>"},{"location":"reference/models/#sequence-encoders","title":"Sequence Encoders","text":""},{"location":"reference/models/#transformersequenceencoder","title":"<code>TransformerSequenceEncoder</code>","text":"<p>Transformer-based sequence encoder using self-attention.</p> <pre><code>from tabular_ssl.models.components import TransformerSequenceEncoder\n</code></pre> <p>Constructor Parameters: - <code>input_dim</code> (int): Input dimension - <code>hidden_dim</code> (int): Hidden dimension - <code>num_layers</code> (int): Number of transformer layers - <code>num_heads</code> (int): Number of attention heads - <code>dim_feedforward</code> (int): Feedforward network dimension - <code>dropout</code> (float): Dropout rate - <code>max_seq_length</code> (int): Maximum sequence length for positional encoding</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.TransformerSequenceEncoder\ninput_dim: 512\nhidden_dim: 512\nnum_layers: 4\nnum_heads: 8\ndim_feedforward: 2048\ndropout: 0.1\nmax_seq_length: 2048\n</code></pre></p>"},{"location":"reference/models/#s4sequenceencoder","title":"<code>S4SequenceEncoder</code>","text":"<p>Structured State Space (S4) model for efficient long sequence processing.</p> <pre><code>from tabular_ssl.models.components import S4SequenceEncoder\n</code></pre> <p>Constructor Parameters: - <code>input_dim</code> (int): Input dimension - <code>hidden_dim</code> (int): Hidden state dimension - <code>num_layers</code> (int): Number of S4 layers - <code>dropout</code> (float): Dropout rate - <code>bidirectional</code> (bool): Whether to use bidirectional processing - <code>max_sequence_length</code> (int): Maximum sequence length</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.S4SequenceEncoder\ninput_dim: 512\nhidden_dim: 64\nnum_layers: 2\ndropout: 0.1\nbidirectional: true\nmax_sequence_length: 2048\n</code></pre></p>"},{"location":"reference/models/#rnnsequenceencoder","title":"<code>RNNSequenceEncoder</code>","text":"<p>RNN-based sequence encoder (LSTM, GRU, or vanilla RNN).</p> <pre><code>from tabular_ssl.models.components import RNNSequenceEncoder\n</code></pre> <p>Constructor Parameters: - <code>input_dim</code> (int): Input dimension - <code>hidden_dim</code> (int): Hidden dimension - <code>num_layers</code> (int): Number of RNN layers - <code>rnn_type</code> (str): Type of RNN (\"lstm\", \"gru\", \"rnn\") - <code>dropout</code> (float): Dropout rate - <code>bidirectional</code> (bool): Whether to use bidirectional RNN</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.RNNSequenceEncoder\ninput_dim: 128\nhidden_dim: 128\nnum_layers: 2\nrnn_type: lstm\ndropout: 0.1\nbidirectional: false\n</code></pre></p>"},{"location":"reference/models/#embedding-layers","title":"Embedding Layers","text":""},{"location":"reference/models/#categoricalembedding","title":"<code>CategoricalEmbedding</code>","text":"<p>Handles embedding of categorical features with flexible dimensions.</p> <pre><code>from tabular_ssl.models.components import CategoricalEmbedding\n</code></pre> <p>Constructor Parameters: - <code>categorical_features</code> (List[Dict]): List of categorical feature specifications - <code>default_embedding_dim</code> (int): Default embedding dimension - <code>categorical_embedding_dims</code> (Dict[str, int]): Custom dimensions per feature</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.CategoricalEmbedding\ncategorical_features:\n  - name: category_1\n    num_categories: 10\n    embedding_dim: 32\n  - name: category_2\n    num_categories: 100\n    embedding_dim: 64\n</code></pre></p>"},{"location":"reference/models/#projection-heads","title":"Projection Heads","text":""},{"location":"reference/models/#mlpprojectionhead","title":"<code>MLPProjectionHead</code>","text":"<p>MLP-based projection head for transforming representations.</p> <pre><code>from tabular_ssl.models.components import MLPProjectionHead\n</code></pre> <p>Constructor Parameters: - <code>input_dim</code> (int): Input dimension - <code>hidden_dims</code> (List[int]): Hidden layer dimensions - <code>output_dim</code> (int): Output dimension - <code>dropout</code> (float): Dropout rate - <code>activation</code> (str): Activation function</p>"},{"location":"reference/models/#prediction-heads","title":"Prediction Heads","text":""},{"location":"reference/models/#classificationhead","title":"<code>ClassificationHead</code>","text":"<p>Classification head for supervised learning tasks.</p> <pre><code>from tabular_ssl.models.components import ClassificationHead\n</code></pre> <p>Constructor Parameters: - <code>input_dim</code> (int): Input dimension - <code>num_classes</code> (int): Number of output classes - <code>hidden_dims</code> (List[int]): Optional hidden layers - <code>dropout</code> (float): Dropout rate</p>"},{"location":"reference/models/#corruption-strategies","title":"Corruption Strategies","text":"<p>Corruption strategies are essential components for self-supervised learning on tabular data. They create pretext tasks by transforming input data.</p>"},{"location":"reference/models/#vimecorruption","title":"<code>VIMECorruption</code>","text":"<p>VIME (Value Imputation and Mask Estimation) corruption strategy.</p> <pre><code>from tabular_ssl.models.components import VIMECorruption\n</code></pre> <p>Constructor Parameters: - <code>corruption_rate</code> (float): Fraction of features to corrupt (default: 0.15) - <code>categorical_indices</code> (List[int]): Indices of categorical features - <code>numerical_indices</code> (List[int]): Indices of numerical features - <code>categorical_vocab_sizes</code> (Dict[int, int]): Vocabulary sizes per categorical feature - <code>numerical_distributions</code> (Dict[int, Tuple[float, float]]): (mean, std) per numerical feature</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.VIMECorruption\ncorruption_rate: 0.3\ncategorical_indices: [0, 1, 2]\nnumerical_indices: [3, 4, 5, 6]\n</code></pre></p>"},{"location":"reference/models/#scarfcorruption","title":"<code>SCARFCorruption</code>","text":"<p>SCARF (Self-Supervised Contrastive Learning using Random Feature Corruption) strategy.</p> <pre><code>from tabular_ssl.models.components import SCARFCorruption\n</code></pre> <p>Constructor Parameters: - <code>corruption_rate</code> (float): Fraction of features to corrupt (default: 0.6) - <code>corruption_strategy</code> (str): \"random_swap\" or \"marginal_sampling\"</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.SCARFCorruption\ncorruption_rate: 0.6\ncorruption_strategy: \"random_swap\"\n</code></pre></p>"},{"location":"reference/models/#recontabcorruption","title":"<code>ReConTabCorruption</code>","text":"<p>Multi-task reconstruction-based corruption strategy.</p> <pre><code>from tabular_ssl.models.components import ReConTabCorruption\n</code></pre> <p>Constructor Parameters: - <code>corruption_rate</code> (float): Base corruption rate for masking (default: 0.15) - <code>categorical_indices</code> (List[int]): Indices of categorical features - <code>numerical_indices</code> (List[int]): Indices of numerical features - <code>corruption_types</code> (List[str]): Types of corruption to apply - <code>masking_strategy</code> (str): \"random\", \"column_wise\", or \"block\" - <code>noise_std</code> (float): Standard deviation for noise corruption - <code>swap_probability</code> (float): Probability of feature swapping</p> <p>Example Configuration: <pre><code>_target_: tabular_ssl.models.components.ReConTabCorruption\ncorruption_rate: 0.15\ncorruption_types: [\"masking\", \"noise\", \"swapping\"]\nmasking_strategy: \"random\"\nnoise_std: 0.1\n</code></pre></p>"},{"location":"reference/models/#simple-corruption-strategies","title":"Simple Corruption Strategies","text":""},{"location":"reference/models/#randommasking","title":"<code>RandomMasking</code>","text":"<pre><code>_target_: tabular_ssl.models.components.RandomMasking\ncorruption_rate: 0.15\n</code></pre>"},{"location":"reference/models/#gaussiannoise","title":"<code>GaussianNoise</code>","text":"<pre><code>_target_: tabular_ssl.models.components.GaussianNoise\nnoise_std: 0.1\n</code></pre>"},{"location":"reference/models/#swappingcorruption","title":"<code>SwappingCorruption</code>","text":"<pre><code>_target_: tabular_ssl.models.components.SwappingCorruption\nswap_prob: 0.15\n</code></pre>"},{"location":"reference/models/#utility-functions","title":"Utility Functions","text":""},{"location":"reference/models/#create_mlp","title":"<code>create_mlp</code>","text":"<p>Utility function for creating MLP layers.</p> <pre><code>from tabular_ssl.models.base import create_mlp\n\nmlp = create_mlp(\n    input_dim=64,\n    hidden_dims=[128, 256],\n    output_dim=512,\n    dropout=0.1,\n    activation=\"relu\",\n    use_batch_norm=True\n)\n</code></pre>"},{"location":"reference/models/#component-instantiation","title":"Component Instantiation","text":"<p>Components are instantiated using Hydra's <code>_target_</code> mechanism:</p> <pre><code># Direct instantiation\nencoder = hydra.utils.instantiate({\n    \"_target_\": \"tabular_ssl.models.components.MLPEventEncoder\",\n    \"input_dim\": 64,\n    \"hidden_dims\": [128, 256],\n    \"output_dim\": 512\n})\n\n# From configuration\nencoder = hydra.utils.instantiate(config.model.event_encoder)\n</code></pre>"},{"location":"reference/models/#model-assembly","title":"Model Assembly","text":"<p>The <code>BaseModel</code> class assembles all components:</p> <pre><code># configs/model/default.yaml\ndefaults:\n  - event_encoder: mlp\n  - sequence_encoder: transformer\n  - projection_head: mlp\n  - prediction_head: classification\n\n_target_: tabular_ssl.models.base.BaseModel\nlearning_rate: 1.0e-4\nweight_decay: 0.01\noptimizer_type: adamw\nscheduler_type: cosine\n</code></pre>"},{"location":"reference/models/#creating-custom-components","title":"Creating Custom Components","text":"<p>To create custom components, inherit from the appropriate base class:</p> <pre><code>from tabular_ssl.models.base import EventEncoder\n\nclass CustomEventEncoder(EventEncoder):\n    def __init__(self, input_dim: int, output_dim: int, **kwargs):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear(x.reshape(-1, x.size(-1))).reshape(x.shape[:-1] + (-1,))\n</code></pre> <p>Then create a configuration file:</p> <pre><code># configs/model/event_encoder/custom.yaml\n_target_: path.to.your.CustomEventEncoder\ninput_dim: 64\noutput_dim: 128\n</code></pre>"},{"location":"reference/models/#best-practices","title":"Best Practices","text":"<ol> <li>Component Compatibility: Ensure output dimensions of one component match input dimensions of the next</li> <li>Memory Management: Use appropriate batch sizes and sequence lengths for your hardware</li> <li>Hyperparameter Tuning: Start with provided experiment configurations and adjust as needed</li> <li>Testing: Test custom components with different input shapes before integration</li> </ol>"},{"location":"reference/models/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/models/#mlp-only-model","title":"MLP-Only Model","text":"<pre><code>defaults:\n  - event_encoder: mlp\n  - sequence_encoder: null  # No sequence processing\n</code></pre>"},{"location":"reference/models/#transformer-model","title":"Transformer Model","text":"<pre><code>defaults:\n  - event_encoder: mlp\n  - sequence_encoder: transformer\n</code></pre>"},{"location":"reference/models/#long-sequence-model","title":"Long Sequence Model","text":"<pre><code>defaults:\n  - event_encoder: mlp\n  - sequence_encoder: s4  # Efficient for long sequences\n</code></pre>"},{"location":"reference/models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/models/#dimension-mismatches","title":"Dimension Mismatches","text":"<p>Check that component dimensions are compatible: <pre><code>assert event_encoder.output_dim == sequence_encoder.input_dim\n</code></pre></p>"},{"location":"reference/models/#memory-issues","title":"Memory Issues","text":"<ul> <li>Reduce batch size or sequence length</li> <li>Use gradient accumulation</li> <li>Enable mixed precision training</li> </ul>"},{"location":"reference/models/#training-instability","title":"Training Instability","text":"<ul> <li>Lower learning rate</li> <li>Add gradient clipping</li> <li>Use layer normalization or batch normalization </li> </ul>"},{"location":"reference/utils/","title":"Utility Functions Reference","text":"<p>This section provides detailed documentation of the utility functions and tools in Tabular SSL.</p>"},{"location":"reference/utils/#evaluation-utilities","title":"Evaluation Utilities","text":""},{"location":"reference/utils/#model-evaluation","title":"Model Evaluation","text":"<pre><code>from tabular_ssl.utils import evaluate_model\n\nmetrics = evaluate_model(\n    model,\n    test_data,\n    metrics=['accuracy', 'f1', 'precision', 'recall']\n)\n</code></pre>"},{"location":"reference/utils/#cross-validation","title":"Cross-Validation","text":"<pre><code>from tabular_ssl.utils import cross_validate\n\ncv_results = cross_validate(\n    model,\n    data,\n    n_splits=5,\n    metrics=['accuracy', 'f1']\n)\n</code></pre>"},{"location":"reference/utils/#visualization-utilities","title":"Visualization Utilities","text":""},{"location":"reference/utils/#training-history","title":"Training History","text":"<pre><code>from tabular_ssl.utils import plot_training_history\n\nfig = plot_training_history(history)\nfig.show()\n</code></pre>"},{"location":"reference/utils/#performance-plots","title":"Performance Plots","text":"<pre><code>from tabular_ssl.utils import plot_performance\n\nfig = plot_performance(\n    model,\n    test_data,\n    plot_types=['confusion_matrix', 'roc_curve']\n)\nfig.show()\n</code></pre>"},{"location":"reference/utils/#model-interpretation","title":"Model Interpretation","text":""},{"location":"reference/utils/#feature-importance","title":"Feature Importance","text":"<pre><code>from tabular_ssl.utils import get_feature_importance\n\nimportance = get_feature_importance(model, test_data)\n</code></pre>"},{"location":"reference/utils/#shap-values","title":"SHAP Values","text":"<pre><code>from tabular_ssl.utils import get_shap_values\n\nshap_values = get_shap_values(model, test_data)\n</code></pre>"},{"location":"reference/utils/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"reference/utils/#grid-search","title":"Grid Search","text":"<pre><code>from tabular_ssl.utils import grid_search\n\nbest_params = grid_search(\n    model_class=TabularSSL,\n    param_grid={\n        'hidden_dim': [128, 256, 512],\n        'num_layers': [2, 4, 6]\n    },\n    train_data=train_data,\n    val_data=val_data\n)\n</code></pre>"},{"location":"reference/utils/#random-search","title":"Random Search","text":"<pre><code>from tabular_ssl.utils import random_search\n\nbest_params = random_search(\n    model_class=TabularSSL,\n    param_distributions={\n        'hidden_dim': [128, 256, 512],\n        'num_layers': [2, 4, 6]\n    },\n    train_data=train_data,\n    val_data=val_data,\n    n_iter=10\n)\n</code></pre>"},{"location":"reference/utils/#data-utilities","title":"Data Utilities","text":""},{"location":"reference/utils/#data-validation","title":"Data Validation","text":"<pre><code>from tabular_ssl.utils import validate_data\n\nvalidation_results = validate_data(data)\n</code></pre>"},{"location":"reference/utils/#feature-selection","title":"Feature Selection","text":"<pre><code>from tabular_ssl.utils import select_features\n\nselected_features = select_features(\n    data,\n    target_col='target',\n    method='importance',\n    threshold=0.01\n)\n</code></pre>"},{"location":"reference/utils/#model-utilities","title":"Model Utilities","text":""},{"location":"reference/utils/#model-saving","title":"Model Saving","text":"<pre><code>from tabular_ssl.utils import save_model\n\nsave_model(model, 'model.pt')\n</code></pre>"},{"location":"reference/utils/#model-loading","title":"Model Loading","text":"<pre><code>from tabular_ssl.utils import load_model\n\nmodel = load_model('model.pt')\n</code></pre>"},{"location":"reference/utils/#training-utilities","title":"Training Utilities","text":""},{"location":"reference/utils/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<pre><code>from tabular_ssl.utils import get_lr_scheduler\n\nscheduler = get_lr_scheduler(\n    initial_lr=1e-3,\n    scheduler_type='cosine',\n    warmup_epochs=5\n)\n</code></pre>"},{"location":"reference/utils/#early-stopping","title":"Early Stopping","text":"<pre><code>from tabular_ssl.utils import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    patience=10,\n    min_delta=0.001\n)\n</code></pre>"},{"location":"reference/utils/#common-functions","title":"Common Functions","text":""},{"location":"reference/utils/#metrics","title":"Metrics","text":"<pre><code>from tabular_ssl.utils import (\n    accuracy_score,\n    f1_score,\n    precision_score,\n    recall_score\n)\n\n# Compute metrics\nacc = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n</code></pre>"},{"location":"reference/utils/#data-processing","title":"Data Processing","text":"<pre><code>from tabular_ssl.utils import (\n    normalize_data,\n    encode_categorical,\n    handle_missing\n)\n\n# Process data\nnormalized_data = normalize_data(data)\nencoded_data = encode_categorical(data, categorical_cols)\ncleaned_data = handle_missing(data, strategy='mean')\n</code></pre>"},{"location":"reference/utils/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate evaluation metrics</li> <li>Implement proper cross-validation</li> <li>Visualize results for better understanding</li> <li>Document utility function usage</li> <li>Handle errors gracefully</li> <li>Use type hints for better code clarity</li> <li>Add proper docstrings</li> <li>Include examples in documentation</li> </ol>"},{"location":"reference/utils/#related-resources","title":"Related Resources","text":"<ul> <li>API Reference - Complete API documentation</li> <li>How-to Guides - Evaluation guides</li> <li>Tutorials - Usage examples </li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the Tabular SSL tutorials section. Here you'll find step-by-step guides to help you get started with the library.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":"<ul> <li>Getting Started - Learn the basics of Tabular SSL</li> <li>Basic Usage - Explore common use cases and patterns</li> </ul>"},{"location":"tutorials/#whats-next","title":"What's Next?","text":"<p>After completing the tutorials, you might want to:</p> <ol> <li>Check out the How-to Guides for practical solutions</li> <li>Read the API Reference for detailed documentation</li> <li>Learn about the SSL Methods in depth </li> </ol>"},{"location":"tutorials/basic-usage/","title":"Basic Usage","text":"<p>This tutorial covers common use cases and patterns for working with Tabular SSL.</p>"},{"location":"tutorials/basic-usage/#working-with-different-data-types","title":"Working with Different Data Types","text":""},{"location":"tutorials/basic-usage/#numerical-data","title":"Numerical Data","text":"<pre><code>import pandas as pd\nfrom tabular_ssl import TabularSSL\nfrom hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\n# Create sample numerical data\ndata = pd.DataFrame({\n    'feature1': [1.0, 2.0, 3.0],\n    'feature2': [4.0, 5.0, 6.0]\n})\n\n# Create configuration\nconfig = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': 2\n})\n\n# Initialize and train model\nmodel = instantiate(config)\nmodel.train(data)\n</code></pre>"},{"location":"tutorials/basic-usage/#categorical-data","title":"Categorical Data","text":"<pre><code># Create sample categorical data\ndata = pd.DataFrame({\n    'category1': ['A', 'B', 'A'],\n    'category2': ['X', 'Y', 'Z']\n})\n\n# Preprocess categorical data\nfrom tabular_ssl.data import DataLoader\ndata_loader = DataLoader()\nprocessed_data = data_loader.preprocess(data, categorical_cols=['category1', 'category2'])\n\n# Create configuration\nconfig = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': processed_data.shape[1]\n})\n\n# Train model\nmodel = instantiate(config)\nmodel.train(processed_data)\n</code></pre>"},{"location":"tutorials/basic-usage/#mixed-data-types","title":"Mixed Data Types","text":"<pre><code># Create sample mixed data\ndata = pd.DataFrame({\n    'numeric': [1.0, 2.0, 3.0],\n    'category': ['A', 'B', 'A']\n})\n\n# Preprocess data\nprocessed_data = data_loader.preprocess(\n    data,\n    categorical_cols=['category']\n)\n\n# Create configuration\nconfig = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': processed_data.shape[1]\n})\n\n# Train model\nmodel = instantiate(config)\nmodel.train(processed_data)\n</code></pre>"},{"location":"tutorials/basic-usage/#model-configuration","title":"Model Configuration","text":""},{"location":"tutorials/basic-usage/#custom-architecture","title":"Custom Architecture","text":"<pre><code># Create configuration with custom architecture\nconfig = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': 10,\n    'sequence_encoder': {\n        '_target_': 'tabular_ssl.models.encoders.TransformerEncoder',\n        'input_dim': 10,\n        'hidden_dim': 512,    # Larger hidden dimension\n        'num_layers': 6,      # More transformer layers\n        'num_heads': 8,       # More attention heads\n        'dropout': 0.2        # Higher dropout\n    },\n    'mask_ratio': 0.2         # Higher masking ratio\n})\n\n# Initialize model\nmodel = instantiate(config)\n</code></pre>"},{"location":"tutorials/basic-usage/#training-configuration","title":"Training Configuration","text":"<pre><code># Create training configuration\ntrain_config = OmegaConf.create({\n    'batch_size': 64,         # Larger batch size\n    'epochs': 200,            # More epochs\n    'learning_rate': 5e-5,    # Lower learning rate\n    'weight_decay': 1e-4      # L2 regularization\n})\n\n# Train model\nhistory = model.train(\n    data=processed_data,\n    **train_config\n)\n</code></pre>"},{"location":"tutorials/basic-usage/#model-evaluation","title":"Model Evaluation","text":""},{"location":"tutorials/basic-usage/#computing-metrics","title":"Computing Metrics","text":"<pre><code>from tabular_ssl.utils import evaluate_model\n\nmetrics = evaluate_model(\n    model,\n    test_data,\n    metrics=['accuracy', 'f1', 'precision', 'recall']\n)\nprint(metrics)\n</code></pre>"},{"location":"tutorials/basic-usage/#visualization","title":"Visualization","text":"<pre><code>from tabular_ssl.utils import plot_training_history\nimport matplotlib.pyplot as plt\n\n# Plot training history\nfig = plot_training_history(history)\nplt.show()\n\n# Save the plot\nfig.savefig('training_history.png')\n</code></pre>"},{"location":"tutorials/basic-usage/#model-persistence","title":"Model Persistence","text":""},{"location":"tutorials/basic-usage/#saving-and-loading","title":"Saving and Loading","text":"<pre><code># Save model\nmodel.save('my_model.pt')\n\n# Load model\nloaded_model = TabularSSL.load('my_model.pt')\n</code></pre>"},{"location":"tutorials/basic-usage/#using-different-encoders","title":"Using Different Encoders","text":""},{"location":"tutorials/basic-usage/#transformer-encoder","title":"Transformer Encoder","text":"<pre><code>config = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': 10,\n    'sequence_encoder': {\n        '_target_': 'tabular_ssl.models.encoders.TransformerEncoder',\n        'input_dim': 10,\n        'hidden_dim': 256,\n        'num_layers': 4,\n        'num_heads': 4\n    }\n})\n</code></pre>"},{"location":"tutorials/basic-usage/#rnn-encoder","title":"RNN Encoder","text":"<pre><code>config = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': 10,\n    'sequence_encoder': {\n        '_target_': 'tabular_ssl.models.encoders.RNNEncoder',\n        'input_dim': 10,\n        'hidden_dim': 256,\n        'num_layers': 2,\n        'bidirectional': true\n    }\n})\n</code></pre>"},{"location":"tutorials/basic-usage/#lstm-encoder","title":"LSTM Encoder","text":"<pre><code>config = OmegaConf.create({\n    '_target_': 'tabular_ssl.models.TabularSSL',\n    'input_dim': 10,\n    'sequence_encoder': {\n        '_target_': 'tabular_ssl.models.encoders.LSTMEncoder',\n        'input_dim': 10,\n        'hidden_dim': 256,\n        'num_layers': 2,\n        'bidirectional': true\n    }\n})\n</code></pre>"},{"location":"tutorials/basic-usage/#understanding-hydra-configuration","title":"Understanding Hydra Configuration","text":""},{"location":"tutorials/basic-usage/#basic-concepts","title":"Basic Concepts","text":"<p>Hydra is a framework for elegantly configuring complex applications. Here are the key concepts:</p> <ol> <li> <p>Configuration Files <pre><code># configs/model/default.yaml\n_target_: tabular_ssl.models.TabularSSL\ninput_dim: 10\nsequence_encoder:\n  _target_: tabular_ssl.models.encoders.TransformerEncoder\n  input_dim: 10\n  hidden_dim: 256\n</code></pre></p> </li> <li> <p>Configuration Groups <pre><code># configs/model/transformer.yaml\n_target_: tabular_ssl.models.encoders.TransformerEncoder\ninput_dim: ${model.input_dim}\nhidden_dim: 256\nnum_heads: 4\n\n# configs/model/rnn.yaml\n_target_: tabular_ssl.models.encoders.RNNEncoder\ninput_dim: ${model.input_dim}\nhidden_dim: 256\nnum_layers: 2\n</code></pre></p> </li> <li> <p>Configuration Composition <pre><code># configs/experiment/transformer_experiment.yaml\ndefaults:\n  - model: transformer\n  - data: default\n  - trainer: default\n</code></pre></p> </li> </ol>"},{"location":"tutorials/basic-usage/#using-hydra-in-code","title":"Using Hydra in Code","text":"<ol> <li> <p>Loading Configurations <pre><code>from hydra import compose, initialize\n\n# Initialize Hydra\nwith initialize(config_path=\"configs\"):\n    # Load default config\n    config = compose(config_name=\"config\")\n\n    # Load specific experiment\n    experiment_config = compose(config_name=\"experiment/transformer_experiment\")\n</code></pre></p> </li> <li> <p>Instantiating Objects <pre><code>from hydra.utils import instantiate\n\n# Create model from config\nmodel = instantiate(config.model)\n\n# Create optimizer\noptimizer = instantiate(config.optimizer, params=model.parameters())\n</code></pre></p> </li> <li> <p>Overriding Configuration <pre><code># Override specific values\nconfig = compose(\n    config_name=\"config\",\n    overrides=[\"model.sequence_encoder.hidden_dim=512\"]\n)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/basic-usage/#advanced-features","title":"Advanced Features","text":"<ol> <li> <p>Variable Interpolation <pre><code># configs/model/default.yaml\ninput_dim: 10\nsequence_encoder:\n  input_dim: ${model.input_dim}  # References parent config\n  hidden_dim: ${oc.env:HIDDEN_DIM,256}  # Uses environment variable with default\n</code></pre></p> </li> <li> <p>Configuration Inheritance <pre><code># configs/model/base.yaml\n_target_: tabular_ssl.models.TabularSSL\ninput_dim: 10\n\n# configs/model/large.yaml\ndefaults:\n  - base\nhidden_dim: 512\nnum_layers: 6\n</code></pre></p> </li> <li> <p>Structured Configs <pre><code>from dataclasses import dataclass\nfrom omegaconf import MISSING\n\n@dataclass\nclass ModelConfig:\n    _target_: str = MISSING\n    input_dim: int = MISSING\n    hidden_dim: int = 256\n\n@dataclass\nclass Config:\n    model: ModelConfig = MISSING\n</code></pre></p> </li> </ol>"},{"location":"tutorials/basic-usage/#best-practices","title":"Best Practices","text":"<ol> <li>Configuration Organization</li> <li>Keep related configs together</li> <li>Use meaningful names</li> <li> <p>Document configuration options</p> </li> <li> <p>Default Values</p> </li> <li>Provide sensible defaults</li> <li>Use type hints</li> <li> <p>Document parameter ranges</p> </li> <li> <p>Error Handling <pre><code>from omegaconf import OmegaConf\n\n# Validate config\ntry:\n    OmegaConf.to_container(config, resolve=True)\nexcept Exception as e:\n    print(f\"Invalid configuration: {e}\")\n</code></pre></p> </li> <li> <p>Configuration Logging <pre><code># Log configuration\nprint(OmegaConf.to_yaml(config))\n\n# Save configuration\nOmegaConf.save(config, \"config.yaml\")\n</code></pre></p> </li> </ol>"},{"location":"tutorials/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore How-to Guides for more specific use cases</li> <li>Check the API Reference for detailed documentation</li> <li>Learn about SSL Methods in depth </li> </ul>"},{"location":"tutorials/custom-components/","title":"Creating Custom Components","text":"<p>Time to complete: 20 minutes</p> <p>In this tutorial, you'll learn how to create your own custom components for Tabular SSL. We'll start with a simple example and gradually build up your understanding.</p>"},{"location":"tutorials/custom-components/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to create a simple custom event encoder</li> <li>How to configure it using YAML files</li> <li>How to test and use your custom component</li> <li>Basic patterns for extending the library</li> </ul>"},{"location":"tutorials/custom-components/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed the Getting Started tutorial</li> <li>Basic PyTorch knowledge</li> <li>Understanding of neural networks</li> </ul>"},{"location":"tutorials/custom-components/#the-goal","title":"The Goal","text":"<p>We'll create a custom event encoder that uses a different activation function and architecture than the default MLP encoder. This will teach you the patterns for creating any type of custom component.</p>"},{"location":"tutorials/custom-components/#step-1-understanding-component-structure","title":"Step 1: Understanding Component Structure","text":"<p>First, let's look at what makes a component in Tabular SSL:</p> <ol> <li>Inherits from a base class (like <code>EventEncoder</code>)</li> <li>Has a constructor with parameters</li> <li>Implements a <code>forward</code> method for processing data</li> <li>Can be configured via YAML files</li> </ol>"},{"location":"tutorials/custom-components/#step-2-create-your-first-custom-component","title":"Step 2: Create Your First Custom Component","text":"<p>Let's create a simple custom event encoder. Create a new file:</p> <pre><code># src/tabular_ssl/models/my_components.py\nimport torch\nimport torch.nn as nn\nfrom typing import List\n\nfrom tabular_ssl.models.base import EventEncoder\n\nclass SimpleCustomEncoder(EventEncoder):\n    \"\"\"A simple custom encoder with GELU activation and layer normalization.\"\"\"\n\n    def __init__(\n        self, \n        input_dim: int,\n        output_dim: int,\n        hidden_dim: int = 128,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n\n        # Store parameters\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n\n        # Create layers\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Process the input tensor.\"\"\"\n        # x shape: (batch_size, seq_len, input_dim)\n        batch_size, seq_len, _ = x.shape\n\n        # Reshape to process all timesteps at once\n        x_flat = x.reshape(-1, self.input_dim)\n\n        # Apply our layers\n        output_flat = self.layers(x_flat)\n\n        # Reshape back to sequence format\n        output = output_flat.reshape(batch_size, seq_len, self.output_dim)\n\n        return output\n</code></pre>"},{"location":"tutorials/custom-components/#step-3-test-your-component","title":"Step 3: Test Your Component","text":"<p>Before using it in training, let's test that it works:</p> <pre><code># test_my_component.py\nimport torch\nfrom tabular_ssl.models.my_components import SimpleCustomEncoder\n\ndef test_custom_encoder():\n    # Create test data\n    batch_size, seq_len, input_dim = 8, 10, 32\n    x = torch.randn(batch_size, seq_len, input_dim)\n\n    # Create encoder\n    encoder = SimpleCustomEncoder(\n        input_dim=32,\n        output_dim=64,\n        hidden_dim=128\n    )\n\n    # Test forward pass\n    output = encoder(x)\n\n    # Check output shape\n    expected_shape = (batch_size, seq_len, 64)\n    assert output.shape == expected_shape, f\"Expected {expected_shape}, got {output.shape}\"\n\n    print(\"\u2705 Custom encoder test passed!\")\n\nif __name__ == \"__main__\":\n    test_custom_encoder()\n</code></pre> <p>Run this test: <pre><code>python test_my_component.py\n</code></pre></p>"},{"location":"tutorials/custom-components/#step-4-create-a-configuration-file","title":"Step 4: Create a Configuration File","text":"<p>Now create a YAML configuration for your component:</p> <pre><code># configs/model/event_encoder/simple_custom.yaml\n_target_: tabular_ssl.models.my_components.SimpleCustomEncoder\n\ninput_dim: 64\noutput_dim: 256\nhidden_dim: 128\ndropout: 0.1\n</code></pre>"},{"location":"tutorials/custom-components/#step-5-use-your-component-in-an-experiment","title":"Step 5: Use Your Component in an Experiment","text":"<p>Create an experiment that uses your custom component:</p> <pre><code># configs/experiments/my_custom_experiment.yaml\n# @package _global_\n\ndefaults:\n  - override /model/event_encoder: simple_custom\n  - override /model/sequence_encoder: null  # Keep it simple for now\n  - override /model/projection_head: mlp\n  - override /model/prediction_head: classification\n\ntags: [\"custom\", \"simple\"]\n\nmodel:\n  learning_rate: 1.0e-3\n  weight_decay: 0.01\n\ntrainer:\n  max_epochs: 5  # Short run for testing\n\ndata:\n  batch_size: 32\n</code></pre>"},{"location":"tutorials/custom-components/#step-6-run-your-custom-experiment","title":"Step 6: Run Your Custom Experiment","text":"<p>First, make sure Python can find your module:</p> <pre><code># Add this to your train.py or create a new script\nimport tabular_ssl.models.my_components  # This imports your custom components\n</code></pre> <p>Then run your experiment:</p> <pre><code>python train.py +experiment=my_custom_experiment\n</code></pre>"},{"location":"tutorials/custom-components/#step-7-compare-with-default","title":"Step 7: Compare with Default","text":"<p>Let's compare your custom component with the default:</p> <pre><code># Run with default MLP encoder\npython train.py +experiment=simple_mlp trainer.max_epochs=5\n\n# Run with your custom encoder  \npython train.py +experiment=my_custom_experiment\n</code></pre> <p>Look at the training logs to see how they perform differently!</p>"},{"location":"tutorials/custom-components/#understanding-what-you-built","title":"Understanding What You Built","text":"<p>Your custom encoder differs from the default in several ways:</p> <ul> <li>GELU activation instead of ReLU</li> <li>Layer normalization instead of batch normalization  </li> <li>Simpler architecture with just one hidden layer</li> <li>Different parameter initialization</li> </ul> <p>These choices can affect training dynamics and final performance.</p>"},{"location":"tutorials/custom-components/#next-steps-making-it-better","title":"Next Steps: Making It Better","text":"<p>Now that you understand the basics, try these improvements:</p>"},{"location":"tutorials/custom-components/#1-add-multiple-hidden-layers","title":"1. Add Multiple Hidden Layers","text":"<pre><code>class BetterCustomEncoder(EventEncoder):\n    def __init__(self, input_dim: int, output_dim: int, hidden_dims: List[int]):\n        super().__init__()\n\n        layers = []\n        dims = [input_dim] + hidden_dims + [output_dim]\n\n        for i in range(len(dims) - 1):\n            layers.extend([\n                nn.Linear(dims[i], dims[i + 1]),\n                nn.LayerNorm(dims[i + 1]),\n                nn.GELU(),\n                nn.Dropout(0.1)\n            ])\n\n        self.layers = nn.Sequential(*layers[:-2])  # Remove last activation and dropout\n</code></pre>"},{"location":"tutorials/custom-components/#2-add-residual-connections","title":"2. Add Residual Connections","text":"<pre><code>class ResidualCustomEncoder(EventEncoder):\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # ... process x ...\n        residual = x if x.size(-1) == output.size(-1) else None\n        if residual is not None:\n            output = output + residual\n        return output\n</code></pre>"},{"location":"tutorials/custom-components/#key-patterns-youve-learned","title":"Key Patterns You've Learned","text":"<ol> <li>Inherit from base classes (<code>EventEncoder</code>, <code>SequenceEncoder</code>, etc.)</li> <li>Use <code>_target_</code> in YAML to specify your component class</li> <li>Test components independently before integration</li> <li>Import your modules so Hydra can find them</li> <li>Handle tensor shapes carefully (especially batch and sequence dimensions)</li> </ol>"},{"location":"tutorials/custom-components/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Forgetting to import your custom module</li> <li>Shape mismatches between components</li> <li>Not testing before using in experiments</li> <li>Complex components that are hard to debug</li> </ul>"},{"location":"tutorials/custom-components/#whats-next","title":"What's Next?","text":"<p>\ud83c\udfaf Ready for more advanced patterns? Check out: - How-to: Advanced Component Patterns - Reference: Component API - Explanation: Architecture Design</p> <p>Congratulations! \ud83c\udf89 You've created your first custom component. You now understand the core patterns for extending Tabular SSL with your own innovations. </p>"},{"location":"tutorials/getting-started/","title":"Getting Started with Tabular SSL","text":"<p>Time to complete: 10 minutes</p> <p>Welcome! This tutorial will get you up and running with Tabular SSL in just a few minutes. You'll explore our interactive demos, understand state-of-the-art corruption strategies, and run your first self-supervised learning experiment.</p>"},{"location":"tutorials/getting-started/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to install and set up Tabular SSL</li> <li>How to explore corruption strategies with interactive demos</li> <li>How to train with real credit card transaction data</li> <li>How to run state-of-the-art SSL experiments (VIME, SCARF, ReConTab)</li> <li>Basic concepts of tabular self-supervised learning</li> </ul>"},{"location":"tutorials/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ </li> <li>Basic familiarity with command line</li> </ul>"},{"location":"tutorials/getting-started/#step-1-installation","title":"Step 1: Installation","text":"<p>Let's start by installing Tabular SSL:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/tabular-ssl.git\ncd tabular-ssl\n\n# Install the package\npip install -e .\n\n# Set up the Python path\nexport PYTHONPATH=$PWD/src\n</code></pre> <p>\u2705 Checkpoint: Verify your installation works: <pre><code>python -c \"import tabular_ssl; print('\u2705 Installation successful!')\"\n</code></pre></p>"},{"location":"tutorials/getting-started/#step-2-explore-corruption-strategies-interactive-demo","title":"Step 2: Explore Corruption Strategies (Interactive Demo)","text":"<p>Before diving into training, let's understand how tabular self-supervised learning works through our interactive demo:</p> <pre><code>python demo_corruption_strategies.py\n</code></pre> <p>This demo shows you: - VIME corruption: Value imputation and mask estimation - SCARF corruption: Contrastive learning with feature swapping - ReConTab corruption: Multi-task reconstruction - Side-by-side comparison of all approaches</p> <p>\u2705 What to expect: You'll see how each corruption strategy transforms data, corruption rates, and example outputs.</p>"},{"location":"tutorials/getting-started/#step-3-try-real-data-credit-card-demo","title":"Step 3: Try Real Data (Credit Card Demo)","text":"<p>Now let's work with real transaction data:</p> <pre><code>python demo_credit_card_data.py\n</code></pre> <p>This demo: - Downloads real credit card transaction data from IBM TabFormer - Shows data preprocessing and sequence creation - Demonstrates DataModule integration - Prepares you for actual training</p> <p>\u2705 What to expect: Download progress, data statistics, and confirmation that everything is ready for training.</p>"},{"location":"tutorials/getting-started/#step-4-your-first-ssl-training","title":"Step 4: Your First SSL Training","text":"<p>Now let's train a state-of-the-art self-supervised model:</p> <pre><code>python train.py +experiment=vime_ssl\n</code></pre> <p>This experiment uses VIME (Value Imputation and Mask Estimation): - Corrupts transaction data by masking features - Learns to predict which features were masked (mask estimation) - Learns to reconstruct original values (value imputation) - Creates powerful representations for downstream tasks</p> <p>\u2705 What to expect: Training progress with mask estimation and reconstruction losses, checkpoints saved to <code>outputs/</code>.</p>"},{"location":"tutorials/getting-started/#step-5-try-different-ssl-methods","title":"Step 5: Try Different SSL Methods","text":"<p>Let's experiment with other state-of-the-art approaches:</p> <pre><code># SCARF: Contrastive learning with feature corruption\npython train.py +experiment=scarf_ssl\n\n# ReConTab: Multi-task reconstruction\npython train.py +experiment=recontab_ssl\n</code></pre> <p>Each approach uses different corruption strategies: - SCARF: Replaces features with values from other samples (contrastive learning) - ReConTab: Combines masking, noise, and swapping (multi-task reconstruction)</p>"},{"location":"tutorials/getting-started/#step-6-customize-ssl-training","title":"Step 6: Customize SSL Training","text":"<p>You can easily modify SSL experiments using Hydra's override syntax:</p> <pre><code># Adjust corruption rate for VIME\npython train.py +experiment=vime_ssl model/corruption.corruption_rate=0.5\n\n# Change SCARF corruption strategy\npython train.py +experiment=scarf_ssl model/corruption.corruption_strategy=marginal_sampling\n\n# Use different sequence length\npython train.py +experiment=recontab_ssl data.sequence_length=64\n\n# Adjust learning rate and batch size\npython train.py +experiment=vime_ssl model.learning_rate=5e-4 data.batch_size=32\n</code></pre>"},{"location":"tutorials/getting-started/#step-7-check-your-results","title":"Step 7: Check Your Results","text":"<p>After running experiments, you'll find results in the <code>outputs/</code> directory:</p> <pre><code>ls outputs/  # See your experiment runs\n</code></pre> <p>Each run creates a timestamped folder with: - Configuration files (reproducing exact settings) - Training logs (TensorBoard, WandB compatible) - Model checkpoints (best performing models) - Metrics and plots (loss curves, validation metrics)</p> <p>For SSL experiments, you'll see specific losses: - VIME: <code>mask_estimation_loss</code>, <code>value_imputation_loss</code> - SCARF: <code>contrastive_loss</code> - ReConTab: <code>masked_reconstruction</code>, <code>denoising</code>, <code>unswapping</code> losses</p>"},{"location":"tutorials/getting-started/#core-concepts-summary","title":"Core Concepts Summary","text":"<p>Corruption Strategies: Core of self-supervised learning - VIME: Masks features, learns to predict masks and values - SCARF: Swaps features between samples for contrastive learning - ReConTab: Multi-task corruption (masking + noise + swapping)</p> <p>SSL Experiments: Pre-configured state-of-the-art approaches - Located in <code>configs/experiments/</code> - Use <code>+experiment=vime_ssl</code> (or scarf_ssl, recontab_ssl) to run them</p> <p>Model Components: Modular architecture - Event encoders (process individual transactions) - Sequence encoders (model temporal patterns) - Corruption modules (transform data for SSL) - Task-specific heads (reconstruction, classification)</p> <p>Configuration: Hydra-based flexible settings - Override corruption rates: <code>model/corruption.corruption_rate=0.5</code> - Mix components: <code>model/sequence_encoder=rnn model/corruption=scarf</code></p>"},{"location":"tutorials/getting-started/#whats-next","title":"What's Next?","text":"<p>\ud83c\udfaf Ready for more? Continue with: - Custom Components Tutorial - Create your own corruption strategies - How-to: SSL Training - Advanced self-supervised learning - Reference: Corruption Strategies - Technical documentation - Reference: Models - Complete component documentation</p>"},{"location":"tutorials/getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors? Make sure PYTHONPATH is set: <pre><code>export PYTHONPATH=$PWD/src\n</code></pre></p> <p>CUDA out of memory? Try smaller batch sizes: <pre><code>python train.py +experiment=simple_mlp data.batch_size=16\n</code></pre></p> <p>Need help? Check our support resources or open an issue on GitHub.</p> <p>Congratulations! \ud83c\udf89 You've successfully run your first Tabular SSL experiments. You're now ready to explore more advanced features and customize the library for your specific needs. </p>"}]}
# @package _global_

# ReConTab Self-Supervised Learning Experiment
# Based on reconstruction-based contrastive learning for tabular data
# Usage: python train.py +experiment=recontab_ssl

defaults:
  - override /data: credit_card
  - override /model/event_encoder: mlp
  - override /model/sequence_encoder: transformer
  - override /model/projection_head: mlp
  - override /model/corruption: recontab

tags: ["recontab", "reconstruction", "contrastive", "ssl"]

# Model configuration for ReConTab
model:
  # Event encoder for processing individual transactions
  event_encoder:
    input_dim: 64
    hidden_dims: [128, 256, 512]
    output_dim: 512
    dropout: 0.15
    activation: gelu
    use_batch_norm: true
    
  # Sequence encoder for temporal dependencies
  sequence_encoder:
    input_dim: 512
    hidden_dim: 512
    num_layers: 4
    num_heads: 8
    dim_feedforward: 1024
    dropout: 0.1
    
  # Projection head for contrastive learning
  projection_head:
    input_dim: 512
    hidden_dims: [256, 128]
    output_dim: 128
    dropout: 0.1
    
  # Reconstruction heads for different corruption types
  reconstruction_heads:
    masked_reconstruction:
      _target_: tabular_ssl.models.components.MLPProjectionHead
      input_dim: 128
      hidden_dims: [256, 512]
      output_dim: ${model.event_encoder.input_dim}
      dropout: 0.1
      
    denoising:
      _target_: tabular_ssl.models.components.MLPProjectionHead
      input_dim: 128
      hidden_dims: [256, 512]
      output_dim: ${model.event_encoder.input_dim}
      dropout: 0.1
      
    unswapping:
      _target_: tabular_ssl.models.components.MLPProjectionHead
      input_dim: 128
      hidden_dims: [256, 512]
      output_dim: ${model.event_encoder.input_dim}
      dropout: 0.1
    
  # ReConTab corruption strategy
  corruption:
    corruption_rate: 0.15
    corruption_types: ["masking", "noise", "swapping"]
    masking_strategy: "random"
    noise_std: 0.1
    swap_probability: 0.1
    
  # Loss configuration
  reconstruction_weights:
    masked_reconstruction: 1.0
    denoising: 0.5
    unswapping: 0.3
    
  contrastive_weight: 0.5
  temperature: 0.1
  
  # Training parameters
  learning_rate: 1e-4
  weight_decay: 0.01
  optimizer_type: adamw
  scheduler_type: cosine

# Data configuration
data:
  sequence_length: 32
  batch_size: 64
  
  # Sample data configuration
  sample_data_config:
    n_users: 1000
    sequence_length: 32

# Training configuration
trainer:
  max_epochs: 150
  gradient_clip_val: 1.0
  precision: 16-mixed
  
  # Validation settings
  val_check_interval: 0.33
  log_every_n_steps: 30
  
  # Enable checkpointing
  enable_checkpointing: true

# Callbacks
callbacks:
  model_checkpoint:
    monitor: "val/total_loss"
    save_top_k: 3
    mode: "min"
  early_stopping:
    monitor: "val/total_loss"
    patience: 25
    min_delta: 0.001

# Logging configuration
logger:
  wandb:
    project: "tabular-ssl-recontab"
    tags: ${tags}
    group: "recontab_experiments" 
# @package _global_

# SCARF Self-Supervised Learning Experiment
# Based on "SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption"
# Usage: python train.py +experiment=scarf_ssl

defaults:
  - override /data: credit_card
  - override /model/event_encoder: mlp
  - override /model/sequence_encoder: transformer
  - override /model/projection_head: mlp
  - override /model/corruption: scarf

tags: ["scarf", "contrastive", "ssl", "feature_corruption"]

# Model configuration for SCARF
model:
  # Event encoder for tabular features
  event_encoder:
    input_dim: 64
    hidden_dims: [128, 256]
    output_dim: 256
    dropout: 0.1
    activation: relu
    use_batch_norm: true
    
  # Sequence encoder for temporal patterns
  sequence_encoder:
    input_dim: 256
    hidden_dim: 256
    num_layers: 2
    num_heads: 8
    dim_feedforward: 512
    dropout: 0.1
    
  # Projection head for contrastive learning
  projection_head:
    input_dim: 256
    hidden_dims: [128]
    output_dim: 128  # Contrastive representation dimension
    dropout: 0.1
    
  # SCARF corruption strategy
  corruption:
    corruption_rate: 0.6
    corruption_strategy: "random_swap"
    
  # Contrastive learning parameters
  temperature: 0.07  # Temperature for InfoNCE loss
  
  # Training parameters
  learning_rate: 3e-4
  weight_decay: 1e-4
  optimizer_type: adamw
  scheduler_type: cosine
  
  # Loss weights
  contrastive_weight: 1.0
  reconstruction_weight: 0.0  # SCARF typically doesn't use reconstruction

# Data configuration
data:
  sequence_length: 32
  batch_size: 128  # Larger batch size for contrastive learning
  
  # Sample data configuration
  sample_data_config:
    n_users: 1500
    sequence_length: 32

# Training configuration
trainer:
  max_epochs: 200
  gradient_clip_val: 1.0
  precision: 16-mixed
  
  # Validation settings
  val_check_interval: 0.5
  log_every_n_steps: 50
  
  # Enable checkpointing
  enable_checkpointing: true

# Callbacks
callbacks:
  model_checkpoint:
    monitor: "val/contrastive_loss"
    save_top_k: 3
    mode: "min"
  early_stopping:
    monitor: "val/contrastive_loss"
    patience: 20
    min_delta: 0.001

# Logging configuration
logger:
  wandb:
    project: "tabular-ssl-scarf"
    tags: ${tags}
    group: "scarf_experiments" 
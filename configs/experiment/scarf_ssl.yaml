# @package _global_

# SCARF SSL Experiment
# Self-Supervised Contrastive Learning using Random Feature Corruption

defaults:
  - override /model: ssl_scarf
  - override /data: credit_card  
  - override /trainer: default
  - override /logger: null
  - override /callbacks: default

# Experiment metadata
tags: ["scarf", "ssl", "contrastive", "feature_corruption"]
experiment_name: "scarf_ssl"

# Data configuration (larger batch for contrastive learning)
data:
  batch_size: 128  # Larger batch size beneficial for contrastive learning
  sequence_length: 32
  use_sample_data: true
  sample_data_config:
    data_source: "credit_card"
    n_users: 1500  # More users for better contrastive learning
  
# Model configuration (SCARF-specific parameters)
model:
  contrastive_temperature: 0.07  # Temperature for InfoNCE loss
  learning_rate: 3.0e-4  # Slightly higher learning rate for SCARF
  weight_decay: 1.0e-4

# Training configuration
trainer:
  max_epochs: 150
  precision: "16-mixed"
  gradient_clip_val: 1.0
  val_check_interval: 0.5
  log_every_n_steps: 50
  
# Callbacks configuration
callbacks:
  model_checkpoint:
    monitor: "val/loss"
    save_top_k: 3
    mode: "min"
  early_stopping:
    monitor: "val/loss"
    patience: 20
    min_delta: 0.001

# Seed for reproducibility
seed: 42 